{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
    "\n",
    "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "Example team list and credits:\n",
    "- Alice Anderson: Conceptualization, Data curation, Methodology, Writing - original draft\n",
    "- Bob Barker:  Analysis, Software, Visualization\n",
    "- Charlie Chang: Project administration, Software, Writing - review & editing\n",
    "- Dani Delgado: Analysis, Background research, Visualization, Writing - original draft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RAW_DATA_DIR = 'data/00-raw/'\n",
    "INT_DATA_DIR = 'data/01-interim/'\n",
    "PROCESSED_DATA_DIR = 'data/02-processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading university_dropout_2.zip:   0%|          | 0.00/15.1M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   0%|          | 5.12k/15.1M [00:00<11:01, 22.9kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   0%|          | 39.9k/15.1M [00:00<01:43, 145kB/s] \u001b[A\n",
      "Downloading university_dropout_2.zip:   0%|          | 60.4k/15.1M [00:00<01:48, 139kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   1%|          | 77.8k/15.1M [00:00<02:03, 121kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   1%|          | 95.2k/15.1M [00:00<01:56, 129kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   1%|          | 121k/15.1M [00:00<01:43, 144kB/s] \u001b[A\n",
      "Downloading university_dropout_2.zip:   1%|          | 147k/15.1M [00:01<01:35, 156kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   1%|          | 172k/15.1M [00:01<01:33, 160kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   1%|▏         | 200k/15.1M [00:01<01:26, 172kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   1%|▏         | 218k/15.1M [00:01<01:36, 154kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   2%|▏         | 234k/15.1M [00:01<01:45, 141kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   2%|▏         | 258k/15.1M [00:01<01:41, 147kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   2%|▏         | 286k/15.1M [00:01<01:33, 159kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   2%|▏         | 310k/15.1M [00:02<01:37, 152kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   2%|▏         | 327k/15.1M [00:02<01:40, 148kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   2%|▏         | 342k/15.1M [00:02<01:41, 146kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   2%|▏         | 372k/15.1M [00:02<01:37, 151kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   3%|▎         | 392k/15.1M [00:02<01:32, 160kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   3%|▎         | 423k/15.1M [00:02<01:30, 162kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   3%|▎         | 456k/15.1M [00:02<01:22, 179kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   3%|▎         | 474k/15.1M [00:03<01:26, 169kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   3%|▎         | 505k/15.1M [00:03<01:23, 176kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   4%|▎         | 538k/15.1M [00:03<01:20, 181kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   4%|▎         | 556k/15.1M [00:03<01:21, 178kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   4%|▍         | 587k/15.1M [00:03<01:19, 182kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   4%|▍         | 605k/15.1M [00:03<01:21, 179kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   4%|▍         | 636k/15.1M [00:04<01:20, 179kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   4%|▍         | 668k/15.1M [00:04<01:18, 185kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   5%|▍         | 700k/15.1M [00:04<01:14, 195kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   5%|▍         | 733k/15.1M [00:04<01:11, 202kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   5%|▌         | 766k/15.1M [00:04<01:11, 200kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   5%|▌         | 799k/15.1M [00:04<01:09, 207kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   5%|▌         | 831k/15.1M [00:04<01:07, 211kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   6%|▌         | 864k/15.1M [00:05<01:06, 215kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   6%|▌         | 897k/15.1M [00:05<01:05, 217kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   6%|▌         | 930k/15.1M [00:05<01:02, 228kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   6%|▋         | 963k/15.1M [00:05<01:02, 227kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   7%|▋         | 995k/15.1M [00:05<01:02, 227kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   7%|▋         | 1.03M/15.1M [00:05<01:00, 232kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   7%|▋         | 1.08M/15.1M [00:05<00:57, 244kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   7%|▋         | 1.11M/15.1M [00:06<00:56, 249kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   8%|▊         | 1.14M/15.1M [00:06<00:56, 249kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   8%|▊         | 1.19M/15.1M [00:06<00:53, 263kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   8%|▊         | 1.22M/15.1M [00:06<00:53, 261kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   8%|▊         | 1.27M/15.1M [00:06<00:50, 273kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   9%|▊         | 1.31M/15.1M [00:06<00:51, 268kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   9%|▉         | 1.35M/15.1M [00:06<00:49, 280kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   9%|▉         | 1.40M/15.1M [00:07<00:46, 293kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:   9%|▉         | 1.44M/15.1M [00:07<00:48, 283kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  10%|▉         | 1.49M/15.1M [00:07<00:46, 297kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  10%|█         | 1.53M/15.1M [00:07<00:45, 302kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  10%|█         | 1.58M/15.1M [00:07<00:43, 311kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  11%|█         | 1.63M/15.1M [00:07<00:41, 325kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  11%|█         | 1.70M/15.1M [00:08<00:39, 337kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  12%|█▏        | 1.75M/15.1M [00:08<00:37, 359kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  12%|█▏        | 1.81M/15.1M [00:08<00:35, 375kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  12%|█▏        | 1.88M/15.1M [00:08<00:33, 396kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  13%|█▎        | 1.94M/15.1M [00:08<00:31, 422kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  13%|█▎        | 2.03M/15.1M [00:08<00:29, 451kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  14%|█▍        | 2.09M/15.1M [00:08<00:30, 424kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  14%|█▍        | 2.17M/15.1M [00:09<00:26, 489kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  15%|█▍        | 2.24M/15.1M [00:09<00:27, 473kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  15%|█▌        | 2.30M/15.1M [00:09<00:27, 463kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  16%|█▌        | 2.37M/15.1M [00:09<00:27, 460kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  16%|█▌        | 2.43M/15.1M [00:09<00:27, 468kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  17%|█▋        | 2.52M/15.1M [00:09<00:26, 477kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  17%|█▋        | 2.58M/15.1M [00:09<00:25, 486kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  18%|█▊        | 2.66M/15.1M [00:10<00:25, 494kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  18%|█▊        | 2.73M/15.1M [00:10<00:24, 513kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  19%|█▊        | 2.81M/15.1M [00:10<00:24, 504kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  19%|█▉        | 2.89M/15.1M [00:10<00:23, 520kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  20%|█▉        | 2.97M/15.1M [00:10<00:22, 531kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  20%|██        | 3.06M/15.1M [00:10<00:21, 560kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  21%|██        | 3.14M/15.1M [00:10<00:21, 559kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  21%|██▏       | 3.22M/15.1M [00:11<00:21, 558kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  22%|██▏       | 3.30M/15.1M [00:11<00:19, 614kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  22%|██▏       | 3.36M/15.1M [00:11<00:19, 612kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  23%|██▎       | 3.43M/15.1M [00:11<00:21, 553kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  23%|██▎       | 3.48M/15.1M [00:11<00:21, 546kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  24%|██▎       | 3.56M/15.1M [00:11<00:20, 565kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  24%|██▍       | 3.64M/15.1M [00:11<00:20, 562kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  25%|██▍       | 3.73M/15.1M [00:11<00:20, 560kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  25%|██▌       | 3.81M/15.1M [00:12<00:18, 618kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  26%|██▌       | 3.87M/15.1M [00:12<00:18, 615kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  26%|██▌       | 3.94M/15.1M [00:12<00:20, 556kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  26%|██▋       | 3.99M/15.1M [00:12<00:20, 548kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  27%|██▋       | 4.07M/15.1M [00:12<00:19, 563kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  27%|██▋       | 4.15M/15.1M [00:12<00:19, 561kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  28%|██▊       | 4.23M/15.1M [00:12<00:19, 559kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  29%|██▊       | 4.32M/15.1M [00:12<00:17, 618kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  29%|██▉       | 4.38M/15.1M [00:13<00:17, 613kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  29%|██▉       | 4.44M/15.1M [00:13<00:19, 554kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  30%|██▉       | 4.51M/15.1M [00:13<00:20, 531kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  30%|███       | 4.59M/15.1M [00:13<00:18, 560kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  31%|███       | 4.66M/15.1M [00:13<00:18, 568kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  31%|███▏      | 4.74M/15.1M [00:13<00:16, 629kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  32%|███▏      | 4.80M/15.1M [00:13<00:16, 622kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  32%|███▏      | 4.87M/15.1M [00:13<00:18, 562kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  33%|███▎      | 4.94M/15.1M [00:14<00:19, 531kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  33%|███▎      | 5.02M/15.1M [00:14<00:17, 567kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  34%|███▎      | 5.10M/15.1M [00:14<00:16, 602kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  34%|███▍      | 5.18M/15.1M [00:14<00:16, 590kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  35%|███▍      | 5.26M/15.1M [00:14<00:17, 579kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  35%|███▌      | 5.36M/15.1M [00:14<00:16, 603kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  36%|███▌      | 5.46M/15.1M [00:14<00:15, 621kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  37%|███▋      | 5.54M/15.1M [00:15<00:15, 605kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  37%|███▋      | 5.64M/15.1M [00:15<00:15, 621kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  38%|███▊      | 5.74M/15.1M [00:15<00:14, 635kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  39%|███▊      | 5.84M/15.1M [00:15<00:14, 645kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  39%|███▉      | 5.93M/15.1M [00:15<00:14, 656kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  40%|███▉      | 6.03M/15.1M [00:15<00:13, 658kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  40%|████      | 6.13M/15.1M [00:15<00:12, 730kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  41%|████      | 6.21M/15.1M [00:15<00:12, 720kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  41%|████▏     | 6.28M/15.1M [00:16<00:12, 690kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  42%|████▏     | 6.36M/15.1M [00:16<00:12, 693kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  43%|████▎     | 6.47M/15.1M [00:16<00:12, 719kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  44%|████▎     | 6.59M/15.1M [00:16<00:11, 740kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  44%|████▍     | 6.72M/15.1M [00:16<00:10, 784kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  45%|████▌     | 6.83M/15.1M [00:16<00:10, 791kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  46%|████▌     | 6.96M/15.1M [00:16<00:09, 822kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  47%|████▋     | 7.09M/15.1M [00:17<00:08, 930kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  48%|████▊     | 7.19M/15.1M [00:17<00:08, 917kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  48%|████▊     | 7.28M/15.1M [00:17<00:08, 891kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  49%|████▉     | 7.39M/15.1M [00:17<00:08, 881kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  50%|████▉     | 7.54M/15.1M [00:17<00:08, 931kB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  51%|█████     | 7.68M/15.1M [00:17<00:06, 1.06MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  51%|█████▏    | 7.79M/15.1M [00:17<00:07, 1.05MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  52%|█████▏    | 7.91M/15.1M [00:17<00:07, 991kB/s] \u001b[A\n",
      "Downloading university_dropout_2.zip:  53%|█████▎    | 8.04M/15.1M [00:17<00:06, 1.07MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  54%|█████▍    | 8.21M/15.1M [00:18<00:06, 1.10MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  55%|█████▌    | 8.37M/15.1M [00:18<00:05, 1.23MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  56%|█████▌    | 8.50M/15.1M [00:18<00:05, 1.23MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  57%|█████▋    | 8.65M/15.1M [00:18<00:05, 1.23MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  58%|█████▊    | 8.80M/15.1M [00:18<00:05, 1.24MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  60%|█████▉    | 9.01M/15.1M [00:18<00:04, 1.32MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  61%|██████    | 9.20M/15.1M [00:18<00:04, 1.48MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  62%|██████▏   | 9.35M/15.1M [00:18<00:03, 1.45MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  63%|██████▎   | 9.53M/15.1M [00:19<00:03, 1.47MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  64%|██████▍   | 9.71M/15.1M [00:19<00:03, 1.49MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  66%|██████▌   | 9.94M/15.1M [00:19<00:03, 1.70MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  67%|██████▋   | 10.1M/15.1M [00:19<00:03, 1.67MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  68%|██████▊   | 10.3M/15.1M [00:19<00:02, 1.66MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  69%|██████▉   | 10.5M/15.1M [00:19<00:02, 1.70MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  71%|███████   | 10.8M/15.1M [00:19<00:02, 1.91MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  72%|███████▏  | 11.0M/15.1M [00:19<00:02, 1.89MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  74%|███████▍  | 11.2M/15.1M [00:19<00:02, 1.91MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  75%|███████▌  | 11.4M/15.1M [00:20<00:01, 1.94MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  77%|███████▋  | 11.7M/15.1M [00:20<00:01, 2.16MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  79%|███████▊  | 11.9M/15.1M [00:20<00:01, 2.13MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  80%|████████  | 12.2M/15.1M [00:20<00:01, 2.19MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  82%|████████▏ | 12.4M/15.1M [00:20<00:01, 2.21MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  84%|████████▍ | 12.8M/15.1M [00:20<00:00, 2.47MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  86%|████████▌ | 13.0M/15.1M [00:20<00:00, 2.45MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  88%|████████▊ | 13.3M/15.1M [00:20<00:00, 2.48MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  90%|████████▉ | 13.6M/15.1M [00:20<00:00, 2.50MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  92%|█████████▏| 14.0M/15.1M [00:21<00:00, 2.81MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  94%|█████████▍| 14.3M/15.1M [00:21<00:00, 2.79MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  97%|█████████▋| 14.6M/15.1M [00:21<00:00, 2.83MB/s]\u001b[A\n",
      "Downloading university_dropout_2.zip:  99%|█████████▊| 14.9M/15.1M [00:21<00:00, 2.87MB/s]\u001b[A\n",
      "Overall Download Progress: 100%|██████████| 1/1 [00:22<00:00, 22.03s/it]                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: university_dropout_2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') \n",
    "\n",
    "import get_data \n",
    "\n",
    "datafiles = [\n",
    "    { 'url': 'https://zenodo.org/records/17239943/files/dataset_2022_hash.zip?download=1', 'filename':'university_dropout_2022.zip'} # Decompressed later using pd.read_csv\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University Student Dropout Dataset\n",
    "\n",
    "The University Student Dropout dataset is organized as yearly CSV files named dataset_{year}.csv, with each row corresponding to a student-course enrollment for that academic year. Each file integrates data from four sources: students, programs, courses, and digital logs, and also groups variables into six thematic categories: context, admission pathways, socio-economic and demographic background, academic data, digital logs, and Wi-Fi access. Contextual attributes include anonymized identifiers for students, courses, academic programs, and campuses, as well as the academic year and group IDs, capturing where and how each student is enrolled. Admission pathway variables describe how the student entered the university, including year of enrollment, type of admission, entry exam grades (scaled to 10 or 14), and program selection preference. Socioeconomic and demographic variables capture parental education, student dedication to studies, and whether the student had to move provinces to attend university, providing insight into economic or social challenges that might affect retention.\n",
    "\n",
    "Academic data is the most detailed category, including grades, credits enrolled and earned across multiple years, semester performance, adjustments for credit recognition, internships, activities, and overall progress toward degree completion. Metrics like cumulative GPA, credits passed per semester, and credit completion rates across previous years allow for longitudinal assessment of academic success and dropout risk. Digital logs track Learning Management System (LMS) site engagement monthly, including number of visits, events, assignment and test submissions, total minutes spent online, and usage of course resources. For 2021 and 2022, Wi-Fi access records provide an additional proxy for on-campus presence, recording the number of days each student accessed the university network per month. All variables are anonymized using hash codes, and numerical metrics such as grades are scaled (e.g., 0–10 or 0–14 for entry exams), while credit counts are in academic credit units. LMS and Wi-Fi activity metrics are counts of actions, logins, or days.\n",
    "\n",
    "While these metrics provide valuable insights, several concerns about the dataset should be noted. It is drawn from a single Spanish technological university, which limits generalizability to other fields or institutions, particularly in humanities or social sciences. Early dropouts may be underrepresented, and some variables, like parental education, employment, student dedication, may be self-reported and incomplete. Engagement measures may also reflect infrastructure availability or device usage rather than actual participation. Finally, identifiers are anonymized, which may reduce the precision of longitudinal tracking, and the data does not include periods affected by the COVID-19 pandemic, meaning it may not capture disruptions caused by virtual or hybrid learning environments. Despite these limitations, the dataset provides a detailed framework for studying factors influencing student retention and academic success.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_804/936537943.py:4: DtypeWarning: Columns (48,57,64,65,164) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data2 = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "\n",
    "# A - Load the Dataset (Just the 2022 Subset for now - it is quite large)\n",
    "data2 = pd.read_csv(\n",
    "    f'{RAW_DATA_DIR}university_dropout_2022.zip',\n",
    "    sep=';',\n",
    "    compression='zip' # Webpage download default as .zip\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the description of the dataset from the source on Zemodo, the dataset has been thouroughly tidied up, which is demontrated below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "==================================================\n",
      "Index(['dni_hash', 'tit_hash', 'asi_hash', 'anyo_ingreso', 'tipo_ingreso',\n",
      "       'nota10_hash', 'nota14_hash', 'campus_hash', 'estudios_p_hash',\n",
      "       'estudios_m_hash',\n",
      "       ...\n",
      "       'n_resource_days_2023_6', 'pft_events_2023_7', 'pft_days_logged_2023_7',\n",
      "       'pft_visits_2023_7', 'pft_assignment_submissions_2023_7',\n",
      "       'pft_test_submissions_2023_7', 'pft_total_minutes_2023_7',\n",
      "       'n_wifi_days_2023_7', 'resource_events_2023_7',\n",
      "       'n_resource_days_2023_7'],\n",
      "      dtype='object', length=169)\n",
      "==================================================\n",
      "dni_hash                       object\n",
      "tit_hash                       object\n",
      "asi_hash                       object\n",
      "anyo_ingreso                   object\n",
      "tipo_ingreso                   object\n",
      "                                ...  \n",
      "pft_test_submissions_2023_7    object\n",
      "pft_total_minutes_2023_7       object\n",
      "n_wifi_days_2023_7             object\n",
      "resource_events_2023_7         object\n",
      "n_resource_days_2023_7         object\n",
      "Length: 169, dtype: object\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dni_hash</th>\n",
       "      <th>tit_hash</th>\n",
       "      <th>asi_hash</th>\n",
       "      <th>anyo_ingreso</th>\n",
       "      <th>tipo_ingreso</th>\n",
       "      <th>nota10_hash</th>\n",
       "      <th>nota14_hash</th>\n",
       "      <th>campus_hash</th>\n",
       "      <th>estudios_p_hash</th>\n",
       "      <th>estudios_m_hash</th>\n",
       "      <th>...</th>\n",
       "      <th>n_resource_days_2023_6</th>\n",
       "      <th>pft_events_2023_7</th>\n",
       "      <th>pft_days_logged_2023_7</th>\n",
       "      <th>pft_visits_2023_7</th>\n",
       "      <th>pft_assignment_submissions_2023_7</th>\n",
       "      <th>pft_test_submissions_2023_7</th>\n",
       "      <th>pft_total_minutes_2023_7</th>\n",
       "      <th>n_wifi_days_2023_7</th>\n",
       "      <th>resource_events_2023_7</th>\n",
       "      <th>n_resource_days_2023_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>4596fcf257c4</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>81f4b5a1d0a8</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>442fcac005ed</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>3dc87ab71825</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>677c622c0bfb</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>2344965e8b89</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>5f52e54c6a9c</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>8b8b029f1142</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>705d739be21c</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>696d9363dc5a</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dni_hash      tit_hash      asi_hash anyo_ingreso tipo_ingreso  \\\n",
       "0  319636fc9270  620c9c332101  4596fcf257c4       2012,0          NAP   \n",
       "1  319636fc9270  620c9c332101  81f4b5a1d0a8       2012,0          NAP   \n",
       "2  319636fc9270  620c9c332101  442fcac005ed       2012,0          NAP   \n",
       "3  319636fc9270  620c9c332101  3dc87ab71825       2012,0          NAP   \n",
       "4  319636fc9270  620c9c332101  677c622c0bfb       2012,0          NAP   \n",
       "5  319636fc9270  620c9c332101  2344965e8b89       2012,0          NAP   \n",
       "6  319636fc9270  620c9c332101  5f52e54c6a9c       2012,0          NAP   \n",
       "7  319636fc9270  620c9c332101  8b8b029f1142       2012,0          NAP   \n",
       "8  319636fc9270  620c9c332101  705d739be21c       2012,0          NAP   \n",
       "9  319636fc9270  620c9c332101  696d9363dc5a       2012,0          NAP   \n",
       "\n",
       "  nota10_hash nota14_hash       campus_hash estudios_p_hash estudios_m_hash  \\\n",
       "0         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "1         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "2         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "3         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "4         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "5         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "6         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "7         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "8         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "9         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "\n",
       "   ... n_resource_days_2023_6 pft_events_2023_7 pft_days_logged_2023_7  \\\n",
       "0  ...                    NaN               NaN                    NaN   \n",
       "1  ...                    NaN               NaN                    NaN   \n",
       "2  ...                    NaN               NaN                    NaN   \n",
       "3  ...                    NaN               NaN                    NaN   \n",
       "4  ...                    NaN               NaN                    NaN   \n",
       "5  ...                    NaN               NaN                    NaN   \n",
       "6  ...                    NaN               NaN                    NaN   \n",
       "7  ...                    NaN               NaN                    NaN   \n",
       "8  ...                    NaN               NaN                    NaN   \n",
       "9  ...                    NaN               NaN                    NaN   \n",
       "\n",
       "  pft_visits_2023_7 pft_assignment_submissions_2023_7  \\\n",
       "0               NaN                               NaN   \n",
       "1               NaN                               NaN   \n",
       "2               NaN                               NaN   \n",
       "3               NaN                               NaN   \n",
       "4               NaN                               NaN   \n",
       "5               NaN                               NaN   \n",
       "6               NaN                               NaN   \n",
       "7               NaN                               NaN   \n",
       "8               NaN                               NaN   \n",
       "9               NaN                               NaN   \n",
       "\n",
       "   pft_test_submissions_2023_7 pft_total_minutes_2023_7 n_wifi_days_2023_7  \\\n",
       "0                          NaN                      NaN                NaN   \n",
       "1                          NaN                      NaN                NaN   \n",
       "2                          NaN                      NaN                NaN   \n",
       "3                          NaN                      NaN                NaN   \n",
       "4                          NaN                      NaN                NaN   \n",
       "5                          NaN                      NaN                NaN   \n",
       "6                          NaN                      NaN                NaN   \n",
       "7                          NaN                      NaN                NaN   \n",
       "8                          NaN                      NaN                NaN   \n",
       "9                          NaN                      NaN                NaN   \n",
       "\n",
       "  resource_events_2023_7 n_resource_days_2023_7  \n",
       "0                    NaN                    NaN  \n",
       "1                    NaN                    NaN  \n",
       "2                    NaN                    NaN  \n",
       "3                    NaN                    NaN  \n",
       "4                    NaN                    NaN  \n",
       "5                    NaN                    NaN  \n",
       "6                    NaN                    NaN  \n",
       "7                    NaN                    NaN  \n",
       "8                    NaN                    NaN  \n",
       "9                    NaN                    NaN  \n",
       "\n",
       "[10 rows x 169 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B - Tidiness\n",
    "\n",
    "# Show that each row is a single observation by cross checking duplicates against the identifiers for student, course, and degree hashes\n",
    "duplicates = data2.duplicated(subset=['dni_hash', 'asi_hash', 'anyo_ingreso'])\n",
    "print(\"Number of duplicate rows:\", duplicates.sum())\n",
    "\n",
    "\n",
    "# Show that columns are aptly named\n",
    "print('='*50)\n",
    "print(data2.columns)\n",
    "print('='*50)\n",
    "print(data2.dtypes) # note that for now, dtypes are often objects because pandas interprets the comma usage in certain numbers as a string most likely\n",
    "\n",
    "# Show a preview of what the data looks like, demonstrating that columns are properly named, there are no overlapping values, and columns are generally meaningful\n",
    "print('='*50)\n",
    "data2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape (rows, columns): (159173, 169)\n",
      "Number of observations of student-course-year (rows): 159173\n",
      "Number of variables (columns): 169\n"
     ]
    }
   ],
   "source": [
    "# C - Size of Dataset\n",
    "print(\"Dataset shape (rows, columns):\", data2.shape)\n",
    "print(\"Number of observations of student-course-year (rows):\", data2.shape[0])\n",
    "print(\"Number of variables (columns):\", data2.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As also mentioned in the paper connected to this dataset, there is a high systematic relationship in the missingness of much of the data, as well as a large portion of columns that have a lot of missing data. This is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pft_test_submissions_2023_7</th>\n",
       "      <td>159148</td>\n",
       "      <td>99.984294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pft_assignment_submissions_2023_7</th>\n",
       "      <td>158800</td>\n",
       "      <td>99.765664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es_retitulado</th>\n",
       "      <td>158630</td>\n",
       "      <td>99.658862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total1</th>\n",
       "      <td>157656</td>\n",
       "      <td>99.046949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es_adaptado</th>\n",
       "      <td>156916</td>\n",
       "      <td>98.582046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rendimiento_cuat_a</th>\n",
       "      <td>12207</td>\n",
       "      <td>7.669014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rendimiento_cuat_b</th>\n",
       "      <td>9051</td>\n",
       "      <td>5.686266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rendimiento_total</th>\n",
       "      <td>8819</td>\n",
       "      <td>5.540513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estudios_m_hash</th>\n",
       "      <td>918</td>\n",
       "      <td>0.576731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estudios_p_hash</th>\n",
       "      <td>918</td>\n",
       "      <td>0.576731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   missing_count  missing_pct\n",
       "pft_test_submissions_2023_7               159148    99.984294\n",
       "pft_assignment_submissions_2023_7         158800    99.765664\n",
       "es_retitulado                             158630    99.658862\n",
       "total1                                    157656    99.046949\n",
       "es_adaptado                               156916    98.582046\n",
       "...                                          ...          ...\n",
       "rendimiento_cuat_a                         12207     7.669014\n",
       "rendimiento_cuat_b                          9051     5.686266\n",
       "rendimiento_total                           8819     5.540513\n",
       "estudios_m_hash                              918     0.576731\n",
       "estudios_p_hash                              918     0.576731\n",
       "\n",
       "[124 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D - Missing Data Exploration\n",
    "# Basic exploratory analysis on the missing data as porportions and counts\n",
    "missing_counts = data2.isnull().sum()\n",
    "missing_percent = (missing_counts / len(data2)) * 100\n",
    "\n",
    "missing_df = pd.concat([missing_counts, missing_percent], axis=1)\n",
    "missing_df.columns = ['missing_count', 'missing_pct']\n",
    "\n",
    "missing_df = missing_df[missing_df['missing_count'] > 0].sort_values(by='missing_pct', ascending=False)\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "campus_hash\n",
       "1398b376fdcce25c    88.458559\n",
       "297c138806bdb5dd    87.812500\n",
       "9103a6c82e355433    85.704161\n",
       "3ca0e4af1c44f084    84.429455\n",
       "7b778e4c1d1f33c9    83.958427\n",
       "0f01a84bff1b2bf4    82.044018\n",
       "60f19cd67252161d    79.815005\n",
       "85ff657216cc9b54    79.775281\n",
       "1a9d786be0ff0bfe    79.341426\n",
       "6781b441c78d2643    78.329399\n",
       "48c6e3d042649ef6    77.824773\n",
       "234001f5d5f1eca4    77.424844\n",
       "f9418773503e50b6    77.080491\n",
       "47cfe5eb8ada0e74    76.700434\n",
       "79df3742da86cfd4    76.659119\n",
       "40f5b57b09f073ed    76.653696\n",
       "86348ea0bf50ebf0    75.927487\n",
       "4e808094851fc2ea    75.469381\n",
       "16a36e86f6fed5d4    75.291622\n",
       "e984139bcc2c5043    75.257732\n",
       "f32b702fba23083f    74.931880\n",
       "5d9d4510699dac58    74.718222\n",
       "911ac1b13dac6fe9    73.259053\n",
       "ddf9288fd8062579    72.413793\n",
       "0672d49fe5a7035e    72.110665\n",
       "eb074cd8374ba297    71.810089\n",
       "f2a369a3b17169d7    71.753555\n",
       "52025890fa603dbc    70.521364\n",
       "2f4c06aba0f9a393    70.130678\n",
       "0448d563bf72277a    70.024096\n",
       "8138689887e6817e    68.799798\n",
       "c8361f9b468e68c8    65.359477\n",
       "e4f95d56d90df35e    64.516339\n",
       "f01a95a004153cb8    63.482414\n",
       "474ffa8c8cb7e88e    63.478261\n",
       "e176f557f5261788    62.251149\n",
       "5abe6ed555720a3e    61.641221\n",
       "8b336cdf52ac9b75    59.274194\n",
       "dae96fc0046a5ae1    52.682927\n",
       "bc5d84bed7dee3e1    38.461538\n",
       "Name: n_wifi_days_2023_7, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D - Missing Data Exploration\n",
    "# A deeper dive into why some data is missing in the way it is\n",
    "# Let us take a look at the missing wifi monitoring usage by campus\n",
    "data2.groupby('campus_hash')['n_wifi_days_2023_7'] \\\n",
    "     .apply(lambda x: x.isnull().mean()*100) \\\n",
    "     .sort_values(ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly demonstrates that some campuses such as `bc5d84bed7dee3e1` have a very comparitively low missing percentage (38%) of their students' wifi utilization, while others, like `1398b376fdcce25c` have a very high missing percentage, around 88%. This clearly shows a systematic disparity in certain campus's ability to report such data. While every inconsistency cannot be described due to the sheer size of this dataset, this small subsample shows how there is a large systematic reason for certain data being missing. This is explored in a little more depth in the accompanying paper, but on a theoretical basis, because this dataset was compiled from various databases and resources and then homogenized, inconsistencies are bound to show. Furthermore, certain courses may be more open to utilizing LMS tools or adopting digital platforms for their education, resulting in systematic missingness in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Discovery\n",
    "\n",
    "The accompanying paper describes that during the data anonymization of students, suspicious variables were dealt with to protect anonymity. For example, if a certain aggregation of variables could identify a student, this was deleted. Furthermore duplicate entries were deleted. Furthermore, a general check to ensure data types remained consistent and that value ranges for data was well within the expected distribution accross datasets was conducted. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "\n",
    "This dataset features a high rate of missingness. As such, the general rule for now that we chose to go with was to delete any columns with a high threshold of missingness. In this case, we chose to drop the columns with more than 90% overall missingness (this means dropping around 30 columns), as even if this data may be useful, the sheer proportion of missing data would make it less impactful. While agressive, this will help us narrow down our scope for our final project. A test also revealed that attempting a super agressive drop of all rows with any sort of missing data would cut the dataset to only 162 entries, so this is also not used. However, entries with all NA entries were deleted. Another notable feature is that the csv was ';' deliminated and utilized commas as decimals, which is quite typical of much of Europe. As such, numbers are cleaned into decimal format and converted to float/int. \n",
    "\n",
    "For specific column-based adjustments, a few rules were established to deal with missingness:\n",
    "\n",
    "1) Leave identifying hashes alone\n",
    "2) Leave demographic/enrollment data alone\n",
    "3) Fill credit/coursework work columns as 0 for NA entries\n",
    "4) Fill activity/practical work columns as 0 for NA entries\n",
    "5) Fill LMS/Wifi/Digital Engagement columns as 0 for NA entries\n",
    "\n",
    "This was done because demographics, enrollement data, and identifying hashes being empty are likely a result of truely missing data. However, the other categories can be attributed to simply the absence of the student doing said column. For example, no entry for credits enrolled for a specific semester and for a specific courses may just mean that the student didn't take that course. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dni_hash</th>\n",
       "      <th>tit_hash</th>\n",
       "      <th>asi_hash</th>\n",
       "      <th>anyo_ingreso</th>\n",
       "      <th>tipo_ingreso</th>\n",
       "      <th>nota10_hash</th>\n",
       "      <th>nota14_hash</th>\n",
       "      <th>campus_hash</th>\n",
       "      <th>estudios_p_hash</th>\n",
       "      <th>estudios_m_hash</th>\n",
       "      <th>...</th>\n",
       "      <th>resource_events_2023_5</th>\n",
       "      <th>n_resource_days_2023_5</th>\n",
       "      <th>pft_events_2023_6</th>\n",
       "      <th>pft_days_logged_2023_6</th>\n",
       "      <th>pft_visits_2023_6</th>\n",
       "      <th>pft_total_minutes_2023_6</th>\n",
       "      <th>n_wifi_days_2023_6</th>\n",
       "      <th>resource_events_2023_6</th>\n",
       "      <th>n_resource_days_2023_6</th>\n",
       "      <th>n_wifi_days_2023_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>4596fcf257c4</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>81f4b5a1d0a8</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>442fcac005ed</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>3dc87ab71825</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>677c622c0bfb</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dni_hash      tit_hash      asi_hash  anyo_ingreso tipo_ingreso  \\\n",
       "0  319636fc9270  620c9c332101  4596fcf257c4        2012.0          NAP   \n",
       "1  319636fc9270  620c9c332101  81f4b5a1d0a8        2012.0          NAP   \n",
       "2  319636fc9270  620c9c332101  442fcac005ed        2012.0          NAP   \n",
       "3  319636fc9270  620c9c332101  3dc87ab71825        2012.0          NAP   \n",
       "4  319636fc9270  620c9c332101  677c622c0bfb        2012.0          NAP   \n",
       "\n",
       "   nota10_hash  nota14_hash       campus_hash estudios_p_hash estudios_m_hash  \\\n",
       "0          NaN        9.456  e4f95d56d90df35e               F               L   \n",
       "1          NaN        9.456  e4f95d56d90df35e               F               L   \n",
       "2          NaN        9.456  e4f95d56d90df35e               F               L   \n",
       "3          NaN        9.456  e4f95d56d90df35e               F               L   \n",
       "4          NaN        9.456  e4f95d56d90df35e               F               L   \n",
       "\n",
       "   ... resource_events_2023_5 n_resource_days_2023_5 pft_events_2023_6  \\\n",
       "0  ...                    0.0                    0.0               0.0   \n",
       "1  ...                    0.0                    0.0               0.0   \n",
       "2  ...                    0.0                    0.0               0.0   \n",
       "3  ...                    0.0                    0.0               0.0   \n",
       "4  ...                    0.0                    0.0               0.0   \n",
       "\n",
       "   pft_days_logged_2023_6 pft_visits_2023_6  pft_total_minutes_2023_6  \\\n",
       "0                     0.0               0.0                       0.0   \n",
       "1                     0.0               0.0                       0.0   \n",
       "2                     0.0               0.0                       0.0   \n",
       "3                     0.0               0.0                       0.0   \n",
       "4                     0.0               0.0                       0.0   \n",
       "\n",
       "  n_wifi_days_2023_6  resource_events_2023_6  n_resource_days_2023_6  \\\n",
       "0                0.0                     0.0                     0.0   \n",
       "1                0.0                     0.0                     0.0   \n",
       "2                0.0                     0.0                     0.0   \n",
       "3                0.0                     0.0                     0.0   \n",
       "4                0.0                     0.0                     0.0   \n",
       "\n",
       "  n_wifi_days_2023_7  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F - Data Cleaning\n",
    "\n",
    "data_cleaned_2 = data2.copy()\n",
    "\n",
    "# Step 1: Drop columns with >90% missingness\n",
    "threshold = 0.90\n",
    "data_cleaned_2 = data_cleaned_2.loc[:, data_cleaned_2.isna().mean() < threshold].copy()\n",
    "\n",
    "# Step 2: Drop rows that are all NA\n",
    "data_cleaned_2 = data_cleaned_2.dropna(axis=0, how='all')\n",
    "\n",
    "# Step 3: Convert comma-based numbers to floats\n",
    "for col in data_cleaned_2.select_dtypes(include='object').columns:\n",
    "    try:\n",
    "        data_cleaned_2[col] = data_cleaned_2[col].str.replace(',', '.').astype(float)\n",
    "    except:\n",
    "        pass  # leave non-numeric columns as object\n",
    "\n",
    "# Step 4: Fill NA with 0 for predetermined count/activity columns\n",
    "fillna_cols = [col for col in data_cleaned_2.columns \n",
    "               if any(x in col for x in ['n_wifi_days', 'resource_events', 'n_resource_days', \n",
    "                                         'pft_', 'actividades', 'total1', 'cred_mat', 'cred_sup'])]\n",
    "\n",
    "for col in fillna_cols:\n",
    "    if pd.api.types.is_numeric_dtype(data_cleaned_2[col]):\n",
    "        data_cleaned_2[col] = data_cleaned_2[col].fillna(0)\n",
    "\n",
    "data_cleaned_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cleaned dataframe: (159173, 135)\n",
      "\n",
      "Data types:\n",
      "float64    115\n",
      "object      13\n",
      "int64        7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cleaned dataset saved to: data/02-processed/university_dropout_cleaned_2022.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of cleaned dataframe:\", data_cleaned_2.shape)\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(data_cleaned_2.dtypes.value_counts())\n",
    "\n",
    "processed_file_path = os.path.join(PROCESSED_DATA_DIR, 'university_dropout_cleaned_2022.csv')\n",
    "data_cleaned_2.to_csv(processed_file_path, index=False, sep=';')\n",
    "\n",
    "print(f\"\\nCleaned dataset saved to: {processed_file_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Replace this with your timeline.  **PLEASE UPDATE your Timeline!** No battle plan survives contact with the enemy, so make sure we understand how your plans have changed.  Also if you have lost points on the previous checkpoint fix them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
