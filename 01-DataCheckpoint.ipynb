{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "- Michelle Ma: Dataset #2\n",
    "- Yves Mojica: Dataset #1\n",
    "- Edgar Seecof: Dataset #1\n",
    "- Travon Williams: Data overview\n",
    "- Felix Xie: Dataset #2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To what extent does first year STEM majors early academic performance predict student dropout at research focused college institutions. Specifically, using students first-year GPA, course completion rates, and credit accumulation as predictors, can we model the probability that a student drops out within one year? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting student dropout in higher education has become a prevalent topic in educational research because early identification of at-risk students can enable universities and colleges to proactively support their students and try to prevent them leaving the college. Student attrition is a large loss to higher education institutions, as it represents lost tuition revenue, reduced completion metrics that reflect poorly on the institution itself, and an inefficient allocation of resources. Arguably, it is sometimes worse off for students, who may incur financial debt, delayed career entry, and negative psychological consequences from leaving college early. At a higher societal level, dropout undermine the workforce and its development, and only further education and economic inequality. \n",
    "\n",
    "This understanding has motivated more and more work to help model student dropout risk using early academic data, serving as a basis for data driven intervention strategies to help mitigate these fallbacks of dropout. Previous research suggests that much of dropout experienced in the first years of college, and is related to students' academic performance early on in their education as it could affect their belief in their academic fit and future success. This is believed to be because early academic success and/or failures act as a feedback loop to update student beliefs in their own abilities. Furthermore, when students fail to meet expectations early on, their assessment of whether or not higher education is a worthwhile financial investment is also called into question, leading to a higher chance of dropout.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1)\n",
    "\n",
    "Building on this theoretical research, researchers in educational data mining have this task of dropout prediction as a supervised machine learning problem. Studies applying traditional ML classification models such as logistic regression, decision trees, and boosting methods have found that academic performance serves as a relatively consistent predictor of dropout (as a binary classification task), amplified when the scope is purely on the first year of dropout. <a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2). This study highlights the feasibility of utilizing a more traditional ML approach to accurately map out dropout rates, and motivates us to focus on early academic indicators. \n",
    "\n",
    "Expanding on this, a similar paper from UCI expands this to a multi-class classification task, adding three labels: graduated, dropped out, or still continuing the degree after the expected amount of time. They found similar results, arguing that early academic performance is a consistent predictor of student outcomes, but they also note that many other factors and variable do play a role, for example one being their financial situation and socioeconomic status. <a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3). This study provides both a validated and cleaned dataset as well as a methodological reference point for modeling dropout. \n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> Stinebrickner, T., & Stinebrickner, R. (2014). A major in science? Initial beliefs and final outcomes for college major and dropout. NBER Working Paper No. 18945. https://www.nber.org/papers/w18945\n",
    "2. <a name=\"cite_note-2\"></a> Lakkaraju, H., Aguiar, E., Shan, C., Miller, D., Bhanpuri, N., Ghani, R., & Addison, K. (2015). A machine learning framework to identify students at risk of adverse academic outcomes. Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. https://dl.acm.org/doi/10.1145/2783258.2788620\n",
    "3. <a name=\"cite_note-3\"></a> Martins, M. V., Tolledo, D., Oliveira, J., & Gonçalves, R. (2021). Early prediction of student’s performance in higher education: A case study. UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/Predict+Students+Dropout+and+Academic+Success"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the data available to us, we predict that there will be a slight correlation between the early academic performance of a student and the likelihood of dropping out of university.  A recurrence of class failure doesn't necessarily mean that a student will inevitably dropout; there are likely other factors that have a greater correlation with early dropout than failing classes in your first year . Students can always catch up, but those who struggle early on will have a harder start to their college career that is more likely to lead to dropping out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "1. Our ideal dataset would include first-year GPA, course completion rate, credit accumulation, and dropout status within one year, documented as a binary variable (0 = student dropped out, 1 = student remained enrolled) to allow for efficient data processing. Depending on the direction of the project, we may also want to incorporate additional variables such as demographics (age, gender, race/ethnicity), socioeconomic factors (family income bracket, first-generation status), high school GPA, field of study, institution type, and campus size. These additional variables would provide more context for each observation and allow us to draw more nuanced conclusions.\n",
    "   \n",
    "    In terms of sample size, we would aim to include several thousand students, ideally over 5,000, to ensure sufficient statistical power and representation of dropout cases. The data would come from undergraduate students entering college for the first time and would be collected using academic records and enrollment statuses during students’ first year of college. These data could be obtained through institutional records, such as registrar data for GPA, credits earned, and course completion, and enrollment databases for registration and withdrawal statuses.\n",
    "\n",
    "    The data should be stored in a clean, tidy, and structured dataset where each row represents a single student and each column represents one variable. Time-based academic variables, such as term GPAs, should be stored in separate columns (e.g., “Fall GPA” and “Spring GPA”). To protect student privacy, each record should also include a unique anonymized student ID rather than personally identifiable information.\n",
    "\n",
    "\n",
    "2. One potential dataset for this project is the Predict Students’ Dropout and Academic Success dataset from the UCI Machine Learning Repository (https://archive.ics.uci.edu/dataset/697/predict+students%27+dropout+and+academic+success). This dataset is publicly available and can be downloaded directly without requesting permission, as it is released under a Creative Commons license. It contains data on approximately 4,400 undergraduate students, including demographic, socioeconomic, and academic performance information collected at enrollment and during the first year. Important variables for this project include semester grades, number of curricular units approved, number of units enrolled, and a categorical outcome variable indicating whether a student dropped out, remained enrolled, or graduated. This outcome variable can be converted into a binary indicator of dropout within one year.\n",
    "\n",
    "    Another useful dataset is the University Student Dropout Longitudinal Dataset hosted on Zenodo and described in an academic data paper (https://zenodo.org/records/17239943). The data are publicly accessible and can be downloaded as CSV files without special permission, though proper citation is required. This dataset tracks students across multiple academic terms and includes detailed information on course enrollments, grades, and credits earned. Key variables relevant to this project include course completion records, cumulative credits earned by term, and enrollment status across semesters, which can be used to derive first-year GPA, completion rates, and dropout within one year. The dataset also includes variables such as parental education level, placement exam results, age, number of assignments submitted, and number of exams taken, which may provide additional context in the analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "Data Overview\n",
    "### Dataset #1\n",
    "\n",
    "- Dataset Name: Predict Students’ Dropout and Academic Success Dataset\n",
    "\n",
    "- Link to dataset: https://archive.ics.uci.edu/dataset/697/predict+students%27+dropout+and+academic+success\n",
    "\n",
    "- Number of observations: 4,424\n",
    "- Number of variables: 37\n",
    "\n",
    "#### Relevant variables:\n",
    "\n",
    "- Academic performance and grades\n",
    "\n",
    "- Student background information\n",
    "\n",
    "- Enrollment characteristics\n",
    "\n",
    "- Dropout or academic success labels\n",
    "\n",
    "#### Shortcomings:\n",
    "\n",
    "- Limited demographic diversity\n",
    "\n",
    "- May not generalize beyond the sampled institutions\n",
    "\n",
    "- Some variables may contain missing or inconsistent values\n",
    "\n",
    "\n",
    "#### Description:\n",
    "\n",
    "- This dataset contains student-level academic and demographic information used to predict dropout and academic success. Each row represents an individual student record with performance and background features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset #2\n",
    "\n",
    "### Dataset Name: University Dropout Dataset (2022)\n",
    "\n",
    "- Link to dataset: https://zenodo.org/records/17239943\n",
    "\n",
    "- Number of observations: 159,173\n",
    "- Number of variables: 169\n",
    "#### Relevant variables:\n",
    "\n",
    "- Academic performance and grades\n",
    "\n",
    "- Learning management system engagement\n",
    "\n",
    "- Campus activity indicators\n",
    "\n",
    "- Student background information\n",
    "\n",
    "#### Shortcomings:\n",
    "\n",
    "- High missingness in engagement variables\n",
    "\n",
    "- Single-institution focus\n",
    "\n",
    "- Differences in grading systems\n",
    "\n",
    "  \n",
    "\n",
    "#### Description:\n",
    "\n",
    "- This dataset contains anonymized student academic and engagement records used to analyze dropout behavior and academic success.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining the datasets\n",
    "\n",
    "\n",
    "Our two datasets will be used to compare patterns between student dropout and their failure of classes in their first year. The datasets come from different sources and structures, but they share common themes being most importantly students academic performance data. We will standardize relevant variables and zero in on trends across the datasets to find the most consistent predictors that lead to dropouts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RAW_DATA_DIR = 'data/00-raw/'\n",
    "INT_DATA_DIR = 'data/01-interim/'\n",
    "PROCESSED_DATA_DIR = 'data/02-processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading predict_students_dropout.zip: 0.00B [00:00, ?B/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: predict_students_dropout.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading university_dropout_2022.zip:   0%|          | 0.00/15.1M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:   0%|          | 12.3k/15.1M [00:00<03:14, 77.9kB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:   0%|          | 39.9k/15.1M [00:00<01:49, 138kB/s] \u001b[A\n",
      "Downloading university_dropout_2022.zip:   1%|          | 96.3k/15.1M [00:00<01:01, 243kB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:   1%|          | 186k/15.1M [00:00<00:39, 381kB/s] \u001b[A\n",
      "Downloading university_dropout_2022.zip:   3%|▎         | 392k/15.1M [00:00<00:20, 731kB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:   5%|▌         | 817k/15.1M [00:00<00:09, 1.43MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  11%|█         | 1.67M/15.1M [00:01<00:04, 2.80MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  13%|█▎        | 1.95M/15.1M [00:01<00:05, 2.47MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  31%|███       | 4.63M/15.1M [00:01<00:01, 7.25MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  39%|███▉      | 5.94M/15.1M [00:01<00:01, 7.68MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  44%|████▍     | 6.71M/15.1M [00:01<00:01, 6.76MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  49%|████▉     | 7.47M/15.1M [00:01<00:01, 6.29MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  58%|█████▊    | 8.80M/15.1M [00:01<00:00, 7.09MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  63%|██████▎   | 9.51M/15.1M [00:02<00:00, 6.30MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  67%|██████▋   | 10.1M/15.1M [00:02<00:00, 5.26MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  71%|███████   | 10.7M/15.1M [00:02<00:00, 5.17MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  74%|███████▍  | 11.2M/15.1M [00:02<00:00, 4.64MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  77%|███████▋  | 11.7M/15.1M [00:02<00:00, 4.18MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  80%|████████  | 12.1M/15.1M [00:02<00:00, 3.78MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  83%|████████▎ | 12.6M/15.1M [00:03<00:00, 3.61MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  86%|████████▌ | 13.0M/15.1M [00:03<00:00, 3.18MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  89%|████████▉ | 13.4M/15.1M [00:03<00:00, 3.24MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  91%|█████████ | 13.8M/15.1M [00:03<00:00, 2.95MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  93%|█████████▎| 14.1M/15.1M [00:03<00:00, 2.45MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  95%|█████████▌| 14.4M/15.1M [00:03<00:00, 2.55MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  97%|█████████▋| 14.7M/15.1M [00:03<00:00, 2.30MB/s]\u001b[A\n",
      "Downloading university_dropout_2022.zip:  99%|█████████▊| 14.9M/15.1M [00:04<00:00, 2.12MB/s]\u001b[A\n",
      "Overall Download Progress: 100%|██████████| 2/2 [00:04<00:00,  2.46s/it]                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: university_dropout_2022.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') \n",
    "\n",
    "import get_data \n",
    "\n",
    "datafiles = [\n",
    "    {\n",
    "        'url': 'https://archive.ics.uci.edu/static/public/697/predict+students+dropout+and+academic+success.zip',\n",
    "        'filename': 'predict_students_dropout.zip'\n",
    "    },\n",
    "    { \n",
    "        'url': 'https://zenodo.org/records/17239943/files/dataset_2022_hash.zip?download=1', \n",
    "        'filename':'university_dropout_2022.zip'\n",
    "    } # Decompressed later using pd.read_csv\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Students’ Dropout and Academic Success dataset\n",
    "\n",
    "The Students' Dropout and Academic Success dataset consists of 1 large CSV file containing a table of data from 4424 responses of students, and 36 different variables/questions. This dataset was created to identify students at risk of dropping out early in their academic career. The variables from this table range from normal metrics such as age or gender to more specific ones such as admission grades or frequency of attendance. For our project, we'll mostly just be looking at the variables related to academic performance, but there are a few that might provide interesting information that we'll also keep an eye on. The link to the data set can be found [here](https://archive.ics.uci.edu/dataset/697/predict+students%27+dropout+and+academic+success)\n",
    "\n",
    "The columns from the dataset we'll for sure be looking at will be the curricular units from the first and second semesters, which include (grade averages, units enrolled, credited, evaluated, not evaluated, and approved), as well as the target column. Early academic performance could also be influenced by other factors, such as previous qualifications or admission grades found in the dataset. The dataset contains information on the number of units a student takes in their first year, split into two semesters. The grade averages are the GPA of a specific student in that specific semester, which just measures the academic performance of a student. The GPA is on the Portuguese scale, so their equivalent of a 4.0 GPA would be a 20.0. A bad gpa would be considered anything less than a 14.0 or the equivalent of a 2.0 in the U.S. A proficient GPA would be between 14.0 and 16.0, which is below a 3.0 GPA. Anything above a 16.0 would just be considered a good GPA. The number of units enrolled is just a numerical value that measures how many credit units/hours a student is taking. This data can be compared to the column with the approved credit units, since that column measures the credits a student earns from successfully passing their courses. The number of evaluations a student takes refers to the number of  exams (evaluations) a student takes in a semester. The number of credited curricular units refers to transfer units from previous coursework. There is a  final column called \"target\", which pretty much lists the final status of these students after conducted for the study, categorical data that lists either \"enrolled\", \"graduate\", or \"drop-out\". These 6 main columns provide some information on the early academic performance of a student through GPA and curricular units, which are about 25 to 30 hours of work per unit. \n",
    "- Enrolled: Number of curricular units being taken in a semester\n",
    "- Credited: Number of transfer curricular units from prior courses\n",
    "- Evaluated: Number of exams taken in curricular units in a semester\n",
    "- Not Evaluated: Number of curricular units without any exams/evaluations\n",
    "- Approved: Number of curricular units passed\n",
    "- Target: Status of student, enrolled, graduate, or dropout\n",
    "\n",
    "Additionally, two columns we could also look at would be previous qualifications, which are just integers that tell us the education level before entering university (a continuous integer scale from 0 to 200 is also in the dataset), and the other column is an admission grade, ranging from 0 to 200, which can also influence early academic performance. A poor admission grade would be below 100, a proficient one would be below 150, and anything higher would be considered a good admission grade reflective of their prior experience. These are all very useful metrics, and the dataset is mostly cleaned out and ready to be used and analyzed by us, but there are a few concerns regarding this dataset. One concern is the fact that all the data is taken from Portuguese students, so things like GPA, course units, and even the overall academic system will differ from what we're used to. This might require making some values more readable, such as a Portuguese GPA conversion to the U.S. system. We'll just have to take account of possible differences in how Portuguese higher education varies from the U.S. system. A smaller concern would be the fact that while the dataset is clean, there are a lot of columns with 0 values that we will not be able to use. So while the dataset is tidy, there are plenty of rows that are incomplete. Besides that, this dataset has a lot of information we can use, and any concerns we have with it can be worked around. \n",
    "\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "## 3.A LOAD DATASET\n",
    "data1 = pd.read_csv(\n",
    "    f'{RAW_DATA_DIR}predict_students_dropout.zip',\n",
    "    sep=';',\n",
    "    compression='zip' # Webpage download default as .zip\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Marital status</th>\n",
       "      <th>Application mode</th>\n",
       "      <th>Application order</th>\n",
       "      <th>Course</th>\n",
       "      <th>Daytime/evening attendance\\t</th>\n",
       "      <th>Previous qualification</th>\n",
       "      <th>Previous qualification (grade)</th>\n",
       "      <th>Nacionality</th>\n",
       "      <th>Mother's qualification</th>\n",
       "      <th>Father's qualification</th>\n",
       "      <th>...</th>\n",
       "      <th>Curricular units 2nd sem (credited)</th>\n",
       "      <th>Curricular units 2nd sem (enrolled)</th>\n",
       "      <th>Curricular units 2nd sem (evaluations)</th>\n",
       "      <th>Curricular units 2nd sem (approved)</th>\n",
       "      <th>Curricular units 2nd sem (grade)</th>\n",
       "      <th>Curricular units 2nd sem (without evaluations)</th>\n",
       "      <th>Unemployment rate</th>\n",
       "      <th>Inflation rate</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9254</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9070</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>9773</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>8014</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>9991</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>133.1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>16.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>14.345000</td>\n",
       "      <td>0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>9254</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>137.0</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>14.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.51</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Marital status  Application mode  Application order  Course  \\\n",
       "0               1                17                  5     171   \n",
       "1               1                15                  1    9254   \n",
       "2               1                 1                  5    9070   \n",
       "3               1                17                  2    9773   \n",
       "4               2                39                  1    8014   \n",
       "5               2                39                  1    9991   \n",
       "6               1                 1                  1    9500   \n",
       "7               1                18                  4    9254   \n",
       "8               1                 1                  3    9238   \n",
       "9               1                 1                  1    9238   \n",
       "\n",
       "   Daytime/evening attendance\\t  Previous qualification  \\\n",
       "0                             1                       1   \n",
       "1                             1                       1   \n",
       "2                             1                       1   \n",
       "3                             1                       1   \n",
       "4                             0                       1   \n",
       "5                             0                      19   \n",
       "6                             1                       1   \n",
       "7                             1                       1   \n",
       "8                             1                       1   \n",
       "9                             1                       1   \n",
       "\n",
       "   Previous qualification (grade)  Nacionality  Mother's qualification  \\\n",
       "0                           122.0            1                      19   \n",
       "1                           160.0            1                       1   \n",
       "2                           122.0            1                      37   \n",
       "3                           122.0            1                      38   \n",
       "4                           100.0            1                      37   \n",
       "5                           133.1            1                      37   \n",
       "6                           142.0            1                      19   \n",
       "7                           119.0            1                      37   \n",
       "8                           137.0           62                       1   \n",
       "9                           138.0            1                       1   \n",
       "\n",
       "   Father's qualification  ...  Curricular units 2nd sem (credited)  \\\n",
       "0                      12  ...                                    0   \n",
       "1                       3  ...                                    0   \n",
       "2                      37  ...                                    0   \n",
       "3                      37  ...                                    0   \n",
       "4                      38  ...                                    0   \n",
       "5                      37  ...                                    0   \n",
       "6                      38  ...                                    0   \n",
       "7                      37  ...                                    0   \n",
       "8                       1  ...                                    0   \n",
       "9                      19  ...                                    0   \n",
       "\n",
       "   Curricular units 2nd sem (enrolled)  \\\n",
       "0                                    0   \n",
       "1                                    6   \n",
       "2                                    6   \n",
       "3                                    6   \n",
       "4                                    6   \n",
       "5                                    5   \n",
       "6                                    8   \n",
       "7                                    5   \n",
       "8                                    6   \n",
       "9                                    6   \n",
       "\n",
       "   Curricular units 2nd sem (evaluations)  \\\n",
       "0                                       0   \n",
       "1                                       6   \n",
       "2                                       0   \n",
       "3                                      10   \n",
       "4                                       6   \n",
       "5                                      17   \n",
       "6                                       8   \n",
       "7                                       5   \n",
       "8                                       7   \n",
       "9                                      14   \n",
       "\n",
       "   Curricular units 2nd sem (approved)  Curricular units 2nd sem (grade)  \\\n",
       "0                                    0                          0.000000   \n",
       "1                                    6                         13.666667   \n",
       "2                                    0                          0.000000   \n",
       "3                                    5                         12.400000   \n",
       "4                                    6                         13.000000   \n",
       "5                                    5                         11.500000   \n",
       "6                                    8                         14.345000   \n",
       "7                                    0                          0.000000   \n",
       "8                                    6                         14.142857   \n",
       "9                                    2                         13.500000   \n",
       "\n",
       "   Curricular units 2nd sem (without evaluations)  Unemployment rate  \\\n",
       "0                                               0               10.8   \n",
       "1                                               0               13.9   \n",
       "2                                               0               10.8   \n",
       "3                                               0                9.4   \n",
       "4                                               0               13.9   \n",
       "5                                               5               16.2   \n",
       "6                                               0               15.5   \n",
       "7                                               0               15.5   \n",
       "8                                               0               16.2   \n",
       "9                                               0                8.9   \n",
       "\n",
       "   Inflation rate   GDP    Target  \n",
       "0             1.4  1.74   Dropout  \n",
       "1            -0.3  0.79  Graduate  \n",
       "2             1.4  1.74   Dropout  \n",
       "3            -0.8 -3.12  Graduate  \n",
       "4            -0.3  0.79  Graduate  \n",
       "5             0.3 -0.92  Graduate  \n",
       "6             2.8 -4.06  Graduate  \n",
       "7             2.8 -4.06   Dropout  \n",
       "8             0.3 -0.92  Graduate  \n",
       "9             1.4  3.51   Dropout  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3.B MAKE TIDY OR SHOW TIDY\n",
    "data1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape (rows, columns): (4424, 37)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Marital status                                      int64\n",
       "Application mode                                    int64\n",
       "Application order                                   int64\n",
       "Course                                              int64\n",
       "Daytime/evening attendance\\t                        int64\n",
       "Previous qualification                              int64\n",
       "Previous qualification (grade)                    float64\n",
       "Nacionality                                         int64\n",
       "Mother's qualification                              int64\n",
       "Father's qualification                              int64\n",
       "Mother's occupation                                 int64\n",
       "Father's occupation                                 int64\n",
       "Admission grade                                   float64\n",
       "Displaced                                           int64\n",
       "Educational special needs                           int64\n",
       "Debtor                                              int64\n",
       "Tuition fees up to date                             int64\n",
       "Gender                                              int64\n",
       "Scholarship holder                                  int64\n",
       "Age at enrollment                                   int64\n",
       "International                                       int64\n",
       "Curricular units 1st sem (credited)                 int64\n",
       "Curricular units 1st sem (enrolled)                 int64\n",
       "Curricular units 1st sem (evaluations)              int64\n",
       "Curricular units 1st sem (approved)                 int64\n",
       "Curricular units 1st sem (grade)                  float64\n",
       "Curricular units 1st sem (without evaluations)      int64\n",
       "Curricular units 2nd sem (credited)                 int64\n",
       "Curricular units 2nd sem (enrolled)                 int64\n",
       "Curricular units 2nd sem (evaluations)              int64\n",
       "Curricular units 2nd sem (approved)                 int64\n",
       "Curricular units 2nd sem (grade)                  float64\n",
       "Curricular units 2nd sem (without evaluations)      int64\n",
       "Unemployment rate                                 float64\n",
       "Inflation rate                                    float64\n",
       "GDP                                               float64\n",
       "Target                                             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3.C DEMONSTRATE SIZE OF DATASET\n",
    "print(\"Dataset shape (rows, columns):\", data1.shape)\n",
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This data set was already used for more formal projects and has already been cleaned of missing values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 missing values.\n"
     ]
    }
   ],
   "source": [
    "## 3.D FIND OUT HOW MUCH DATA IS MISSING AND WHERE\n",
    "col_missing_count = data1.isnull().sum()\n",
    "total_missing_count = col_missing_count.sum()\n",
    "total_missing_count\n",
    "print(f\"There are {total_missing_count} missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers in each column\n",
      "Marital status_outlier                                     505\n",
      "Application mode_outlier                                     0\n",
      "Application order_outlier                                  541\n",
      "Course_outlier                                             442\n",
      "Daytime/evening attendance\\t_outlier                       483\n",
      "Previous qualification_outlier                             707\n",
      "Previous qualification (grade)_outlier                     179\n",
      "Nacionality_outlier                                        110\n",
      "Mother's qualification_outlier                               0\n",
      "Father's qualification_outlier                               0\n",
      "Mother's occupation_outlier                                182\n",
      "Father's occupation_outlier                                177\n",
      "Admission grade_outlier                                     86\n",
      "Displaced_outlier                                            0\n",
      "Educational special needs_outlier                           51\n",
      "Debtor_outlier                                             503\n",
      "Tuition fees up to date_outlier                            528\n",
      "Gender_outlier                                               0\n",
      "Scholarship holder_outlier                                1099\n",
      "Age at enrollment_outlier                                  441\n",
      "International_outlier                                      110\n",
      "Curricular units 1st sem (credited)_outlier                577\n",
      "Curricular units 1st sem (enrolled)_outlier                424\n",
      "Curricular units 1st sem (evaluations)_outlier             158\n",
      "Curricular units 1st sem (approved)_outlier                180\n",
      "Curricular units 1st sem (grade)_outlier                   726\n",
      "Curricular units 1st sem (without evaluations)_outlier     294\n",
      "Curricular units 2nd sem (credited)_outlier                530\n",
      "Curricular units 2nd sem (enrolled)_outlier                369\n",
      "Curricular units 2nd sem (evaluations)_outlier             109\n",
      "Curricular units 2nd sem (approved)_outlier                 44\n",
      "Curricular units 2nd sem (grade)_outlier                   877\n",
      "Curricular units 2nd sem (without evaluations)_outlier     282\n",
      "Unemployment rate_outlier                                    0\n",
      "Inflation rate_outlier                                       0\n",
      "GDP_outlier                                                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## 3.E FIND AND FLAG ANY OUTLIERS OR SUS ENTRIES\n",
    "numeric_cols = data1.select_dtypes(include = ['number']).columns\n",
    "outliers = pd.DataFrame(index = data1.index)\n",
    "## iterate accross all numeric columns and find outliers and store in outliers df\n",
    "for col in numeric_cols:\n",
    "    ##https://stackoverflow.com/questions/23228244/how-do-you-find-the-iqr-in-numpy\n",
    "    q75, q25 = np.nanpercentile(data1[col], [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    lower_bound = q25 - 1.5 * iqr\n",
    "    upper_bound = q75 + 1.5 * iqr\n",
    "    outliers[f'{col}_outlier'] = (data1[col] < lower_bound) | (data1[col] > upper_bound)\n",
    "\n",
    "print('Number of outliers in each column')\n",
    "print(outliers.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This data is already very clean, no missing values or ridiculous outliers, the only thing that we may have to change is the data types being used for certain variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "## 3.F CLEAN THE DATA\n",
    "if data1.isna().sum().sum() == 0:\n",
    "    print('No missing values')\n",
    "else: \n",
    "    print(f'There are {data1.isna().sum().sum()} missin values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.G MOVE TO PROCESSED\n",
    "data1_clean = data1.copy()\n",
    "processed_path = os.path.join(PROCESSED_DATA_DIR, 'predict_students_dropout_clean.csv')\n",
    "data1_clean.to_csv(processed_path, index=False, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Marital status</th>\n",
       "      <th>Application mode</th>\n",
       "      <th>Application order</th>\n",
       "      <th>Course</th>\n",
       "      <th>Daytime/evening attendance\\t</th>\n",
       "      <th>Previous qualification</th>\n",
       "      <th>Previous qualification (grade)</th>\n",
       "      <th>Nacionality</th>\n",
       "      <th>Mother's qualification</th>\n",
       "      <th>Father's qualification</th>\n",
       "      <th>...</th>\n",
       "      <th>Curricular units 1st sem (without evaluations)</th>\n",
       "      <th>Curricular units 2nd sem (credited)</th>\n",
       "      <th>Curricular units 2nd sem (enrolled)</th>\n",
       "      <th>Curricular units 2nd sem (evaluations)</th>\n",
       "      <th>Curricular units 2nd sem (approved)</th>\n",
       "      <th>Curricular units 2nd sem (grade)</th>\n",
       "      <th>Curricular units 2nd sem (without evaluations)</th>\n",
       "      <th>Unemployment rate</th>\n",
       "      <th>Inflation rate</th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.178571</td>\n",
       "      <td>18.669078</td>\n",
       "      <td>1.727848</td>\n",
       "      <td>8856.642631</td>\n",
       "      <td>0.890823</td>\n",
       "      <td>4.577758</td>\n",
       "      <td>132.613314</td>\n",
       "      <td>1.873192</td>\n",
       "      <td>19.561935</td>\n",
       "      <td>22.275316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137658</td>\n",
       "      <td>0.541817</td>\n",
       "      <td>6.232143</td>\n",
       "      <td>8.063291</td>\n",
       "      <td>4.435805</td>\n",
       "      <td>10.230206</td>\n",
       "      <td>0.150316</td>\n",
       "      <td>11.566139</td>\n",
       "      <td>1.228029</td>\n",
       "      <td>0.001969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.605747</td>\n",
       "      <td>17.484682</td>\n",
       "      <td>1.313793</td>\n",
       "      <td>2063.566416</td>\n",
       "      <td>0.311897</td>\n",
       "      <td>10.216592</td>\n",
       "      <td>13.188332</td>\n",
       "      <td>6.914514</td>\n",
       "      <td>15.603186</td>\n",
       "      <td>15.343108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690880</td>\n",
       "      <td>1.918546</td>\n",
       "      <td>2.195951</td>\n",
       "      <td>3.947951</td>\n",
       "      <td>3.014764</td>\n",
       "      <td>5.210808</td>\n",
       "      <td>0.753774</td>\n",
       "      <td>2.663850</td>\n",
       "      <td>1.382711</td>\n",
       "      <td>2.269935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>-4.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9085.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>-1.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9238.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>133.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9556.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9991.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>16.200000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>3.510000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Marital status  Application mode  Application order       Course  \\\n",
       "count     4424.000000       4424.000000        4424.000000  4424.000000   \n",
       "mean         1.178571         18.669078           1.727848  8856.642631   \n",
       "std          0.605747         17.484682           1.313793  2063.566416   \n",
       "min          1.000000          1.000000           0.000000    33.000000   \n",
       "25%          1.000000          1.000000           1.000000  9085.000000   \n",
       "50%          1.000000         17.000000           1.000000  9238.000000   \n",
       "75%          1.000000         39.000000           2.000000  9556.000000   \n",
       "max          6.000000         57.000000           9.000000  9991.000000   \n",
       "\n",
       "       Daytime/evening attendance\\t  Previous qualification  \\\n",
       "count                   4424.000000             4424.000000   \n",
       "mean                       0.890823                4.577758   \n",
       "std                        0.311897               10.216592   \n",
       "min                        0.000000                1.000000   \n",
       "25%                        1.000000                1.000000   \n",
       "50%                        1.000000                1.000000   \n",
       "75%                        1.000000                1.000000   \n",
       "max                        1.000000               43.000000   \n",
       "\n",
       "       Previous qualification (grade)  Nacionality  Mother's qualification  \\\n",
       "count                     4424.000000  4424.000000             4424.000000   \n",
       "mean                       132.613314     1.873192               19.561935   \n",
       "std                         13.188332     6.914514               15.603186   \n",
       "min                         95.000000     1.000000                1.000000   \n",
       "25%                        125.000000     1.000000                2.000000   \n",
       "50%                        133.100000     1.000000               19.000000   \n",
       "75%                        140.000000     1.000000               37.000000   \n",
       "max                        190.000000   109.000000               44.000000   \n",
       "\n",
       "       Father's qualification  ...  \\\n",
       "count             4424.000000  ...   \n",
       "mean                22.275316  ...   \n",
       "std                 15.343108  ...   \n",
       "min                  1.000000  ...   \n",
       "25%                  3.000000  ...   \n",
       "50%                 19.000000  ...   \n",
       "75%                 37.000000  ...   \n",
       "max                 44.000000  ...   \n",
       "\n",
       "       Curricular units 1st sem (without evaluations)  \\\n",
       "count                                     4424.000000   \n",
       "mean                                         0.137658   \n",
       "std                                          0.690880   \n",
       "min                                          0.000000   \n",
       "25%                                          0.000000   \n",
       "50%                                          0.000000   \n",
       "75%                                          0.000000   \n",
       "max                                         12.000000   \n",
       "\n",
       "       Curricular units 2nd sem (credited)  \\\n",
       "count                          4424.000000   \n",
       "mean                              0.541817   \n",
       "std                               1.918546   \n",
       "min                               0.000000   \n",
       "25%                               0.000000   \n",
       "50%                               0.000000   \n",
       "75%                               0.000000   \n",
       "max                              19.000000   \n",
       "\n",
       "       Curricular units 2nd sem (enrolled)  \\\n",
       "count                          4424.000000   \n",
       "mean                              6.232143   \n",
       "std                               2.195951   \n",
       "min                               0.000000   \n",
       "25%                               5.000000   \n",
       "50%                               6.000000   \n",
       "75%                               7.000000   \n",
       "max                              23.000000   \n",
       "\n",
       "       Curricular units 2nd sem (evaluations)  \\\n",
       "count                             4424.000000   \n",
       "mean                                 8.063291   \n",
       "std                                  3.947951   \n",
       "min                                  0.000000   \n",
       "25%                                  6.000000   \n",
       "50%                                  8.000000   \n",
       "75%                                 10.000000   \n",
       "max                                 33.000000   \n",
       "\n",
       "       Curricular units 2nd sem (approved)  Curricular units 2nd sem (grade)  \\\n",
       "count                          4424.000000                       4424.000000   \n",
       "mean                              4.435805                         10.230206   \n",
       "std                               3.014764                          5.210808   \n",
       "min                               0.000000                          0.000000   \n",
       "25%                               2.000000                         10.750000   \n",
       "50%                               5.000000                         12.200000   \n",
       "75%                               6.000000                         13.333333   \n",
       "max                              20.000000                         18.571429   \n",
       "\n",
       "       Curricular units 2nd sem (without evaluations)  Unemployment rate  \\\n",
       "count                                     4424.000000        4424.000000   \n",
       "mean                                         0.150316          11.566139   \n",
       "std                                          0.753774           2.663850   \n",
       "min                                          0.000000           7.600000   \n",
       "25%                                          0.000000           9.400000   \n",
       "50%                                          0.000000          11.100000   \n",
       "75%                                          0.000000          13.900000   \n",
       "max                                         12.000000          16.200000   \n",
       "\n",
       "       Inflation rate          GDP  \n",
       "count     4424.000000  4424.000000  \n",
       "mean         1.228029     0.001969  \n",
       "std          1.382711     2.269935  \n",
       "min         -0.800000    -4.060000  \n",
       "25%          0.300000    -1.700000  \n",
       "50%          1.400000     0.320000  \n",
       "75%          2.600000     1.790000  \n",
       "max          3.700000     3.510000  \n",
       "\n",
       "[8 rows x 36 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 4. SUMMARY STATISTICS\n",
    "data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Previous qualification (grade)  Admission grade  \\\n",
      "0                               2.44            2.546   \n",
      "1                               3.20            2.850   \n",
      "2                               2.44            2.496   \n",
      "3                               2.44            2.392   \n",
      "4                               2.00            2.830   \n",
      "...                              ...              ...   \n",
      "4419                            2.50            2.444   \n",
      "4420                            2.40            2.380   \n",
      "4421                            3.08            2.990   \n",
      "4422                            3.60            3.076   \n",
      "4423                            3.04            3.040   \n",
      "\n",
      "      Curricular units 1st sem (grade)  Curricular units 2nd sem (grade)  \n",
      "0                             0.000000                          0.000000  \n",
      "1                             0.280000                          0.273333  \n",
      "2                             0.000000                          0.000000  \n",
      "3                             0.268571                          0.248000  \n",
      "4                             0.246667                          0.260000  \n",
      "...                                ...                               ...  \n",
      "4419                          0.272000                          0.253333  \n",
      "4420                          0.240000                          0.220000  \n",
      "4421                          0.298250                          0.270000  \n",
      "4422                          0.276000                          0.240000  \n",
      "4423                          0.233333                          0.260000  \n",
      "\n",
      "[4424 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "## 5. TRANSFORMING PORTUGESE GRADING SCALE TO 4.0\n",
    "max_port = 200.0\n",
    "min_port = 0.0\n",
    "max_usa = 4.0\n",
    "\n",
    "grade_cols = [\n",
    "    'Previous qualification (grade)',\n",
    "    'Admission grade',\n",
    "    'Curricular units 1st sem (grade)',\n",
    "    'Curricular units 2nd sem (grade)'\n",
    "]\n",
    "\n",
    "for col in grade_cols:\n",
    "    data1[col] = (data1[col] / max_port) * max_usa\n",
    "\n",
    "print(data1[grade_cols])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University Student Dropout Dataset\n",
    "\n",
    "The University Student Dropout dataset is organized as yearly CSV files named dataset_{year}.csv, with each row corresponding to a student-course enrollment for that academic year. Each file integrates data from four sources: students, programs, courses, and digital logs, and also groups variables into six thematic categories: context, admission pathways, socio-economic and demographic background, academic data, digital logs, and Wi-Fi access. Contextual attributes include anonymized identifiers for students, courses, academic programs, and campuses, as well as the academic year and group IDs, capturing where and how each student is enrolled. Admission pathway variables describe how the student entered the university, including year of enrollment, type of admission, entry exam grades (scaled to 10 or 14), and program selection preference. Socioeconomic and demographic variables capture parental education, student dedication to studies, and whether the student had to move provinces to attend university, providing insight into economic or social challenges that might affect retention.\n",
    "\n",
    "Academic data is the most detailed category, including grades, credits enrolled and earned across multiple years, semester performance, adjustments for credit recognition, internships, activities, and overall progress toward degree completion. Metrics like cumulative GPA, credits passed per semester, and credit completion rates across previous years allow for longitudinal assessment of academic success and dropout risk. Digital logs track Learning Management System (LMS) site engagement monthly, including number of visits, events, assignment and test submissions, total minutes spent online, and usage of course resources. For 2021 and 2022, Wi-Fi access records provide an additional proxy for on-campus presence, recording the number of days each student accessed the university network per month. All variables are anonymized using hash codes, and numerical metrics such as grades are scaled (e.g., 0–10 or 0–14 for entry exams), while credit counts are in academic credit units. LMS and Wi-Fi activity metrics are counts of actions, logins, or days.\n",
    "\n",
    "While these metrics provide valuable insights, several concerns about the dataset should be noted. It is drawn from a single Spanish technological university, which limits generalizability to other fields or institutions, particularly in humanities or social sciences. Early dropouts may be underrepresented, and some variables, like parental education, employment, student dedication, may be self-reported and incomplete. Engagement measures may also reflect infrastructure availability or device usage rather than actual participation. Finally, identifiers are anonymized, which may reduce the precision of longitudinal tracking, and the data does not include periods affected by the COVID-19 pandemic, meaning it may not capture disruptions caused by virtual or hybrid learning environments. Despite these limitations, the dataset provides a detailed framework for studying factors influencing student retention and academic success.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_436/936537943.py:4: DtypeWarning: Columns (48,57,64,65,164) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data2 = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "\n",
    "# A - Load the Dataset (Just the 2022 Subset for now - it is quite large)\n",
    "data2 = pd.read_csv(\n",
    "    f'{RAW_DATA_DIR}university_dropout_2022.zip',\n",
    "    sep=';',\n",
    "    compression='zip' # Webpage download default as .zip\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the description of the dataset from the source on Zemodo, the dataset has been thouroughly tidied up, which is demontrated below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "==================================================\n",
      "Index(['dni_hash', 'tit_hash', 'asi_hash', 'anyo_ingreso', 'tipo_ingreso',\n",
      "       'nota10_hash', 'nota14_hash', 'campus_hash', 'estudios_p_hash',\n",
      "       'estudios_m_hash',\n",
      "       ...\n",
      "       'n_resource_days_2023_6', 'pft_events_2023_7', 'pft_days_logged_2023_7',\n",
      "       'pft_visits_2023_7', 'pft_assignment_submissions_2023_7',\n",
      "       'pft_test_submissions_2023_7', 'pft_total_minutes_2023_7',\n",
      "       'n_wifi_days_2023_7', 'resource_events_2023_7',\n",
      "       'n_resource_days_2023_7'],\n",
      "      dtype='object', length=169)\n",
      "==================================================\n",
      "dni_hash                       object\n",
      "tit_hash                       object\n",
      "asi_hash                       object\n",
      "anyo_ingreso                   object\n",
      "tipo_ingreso                   object\n",
      "                                ...  \n",
      "pft_test_submissions_2023_7    object\n",
      "pft_total_minutes_2023_7       object\n",
      "n_wifi_days_2023_7             object\n",
      "resource_events_2023_7         object\n",
      "n_resource_days_2023_7         object\n",
      "Length: 169, dtype: object\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dni_hash</th>\n",
       "      <th>tit_hash</th>\n",
       "      <th>asi_hash</th>\n",
       "      <th>anyo_ingreso</th>\n",
       "      <th>tipo_ingreso</th>\n",
       "      <th>nota10_hash</th>\n",
       "      <th>nota14_hash</th>\n",
       "      <th>campus_hash</th>\n",
       "      <th>estudios_p_hash</th>\n",
       "      <th>estudios_m_hash</th>\n",
       "      <th>...</th>\n",
       "      <th>n_resource_days_2023_6</th>\n",
       "      <th>pft_events_2023_7</th>\n",
       "      <th>pft_days_logged_2023_7</th>\n",
       "      <th>pft_visits_2023_7</th>\n",
       "      <th>pft_assignment_submissions_2023_7</th>\n",
       "      <th>pft_test_submissions_2023_7</th>\n",
       "      <th>pft_total_minutes_2023_7</th>\n",
       "      <th>n_wifi_days_2023_7</th>\n",
       "      <th>resource_events_2023_7</th>\n",
       "      <th>n_resource_days_2023_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>4596fcf257c4</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>81f4b5a1d0a8</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>442fcac005ed</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>3dc87ab71825</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>677c622c0bfb</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>2344965e8b89</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>5f52e54c6a9c</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>8b8b029f1142</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>705d739be21c</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>696d9363dc5a</td>\n",
       "      <td>2012,0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dni_hash      tit_hash      asi_hash anyo_ingreso tipo_ingreso  \\\n",
       "0  319636fc9270  620c9c332101  4596fcf257c4       2012,0          NAP   \n",
       "1  319636fc9270  620c9c332101  81f4b5a1d0a8       2012,0          NAP   \n",
       "2  319636fc9270  620c9c332101  442fcac005ed       2012,0          NAP   \n",
       "3  319636fc9270  620c9c332101  3dc87ab71825       2012,0          NAP   \n",
       "4  319636fc9270  620c9c332101  677c622c0bfb       2012,0          NAP   \n",
       "5  319636fc9270  620c9c332101  2344965e8b89       2012,0          NAP   \n",
       "6  319636fc9270  620c9c332101  5f52e54c6a9c       2012,0          NAP   \n",
       "7  319636fc9270  620c9c332101  8b8b029f1142       2012,0          NAP   \n",
       "8  319636fc9270  620c9c332101  705d739be21c       2012,0          NAP   \n",
       "9  319636fc9270  620c9c332101  696d9363dc5a       2012,0          NAP   \n",
       "\n",
       "  nota10_hash nota14_hash       campus_hash estudios_p_hash estudios_m_hash  \\\n",
       "0         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "1         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "2         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "3         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "4         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "5         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "6         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "7         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "8         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "9         NaN       9,456  e4f95d56d90df35e               F               L   \n",
       "\n",
       "   ... n_resource_days_2023_6 pft_events_2023_7 pft_days_logged_2023_7  \\\n",
       "0  ...                    NaN               NaN                    NaN   \n",
       "1  ...                    NaN               NaN                    NaN   \n",
       "2  ...                    NaN               NaN                    NaN   \n",
       "3  ...                    NaN               NaN                    NaN   \n",
       "4  ...                    NaN               NaN                    NaN   \n",
       "5  ...                    NaN               NaN                    NaN   \n",
       "6  ...                    NaN               NaN                    NaN   \n",
       "7  ...                    NaN               NaN                    NaN   \n",
       "8  ...                    NaN               NaN                    NaN   \n",
       "9  ...                    NaN               NaN                    NaN   \n",
       "\n",
       "  pft_visits_2023_7 pft_assignment_submissions_2023_7  \\\n",
       "0               NaN                               NaN   \n",
       "1               NaN                               NaN   \n",
       "2               NaN                               NaN   \n",
       "3               NaN                               NaN   \n",
       "4               NaN                               NaN   \n",
       "5               NaN                               NaN   \n",
       "6               NaN                               NaN   \n",
       "7               NaN                               NaN   \n",
       "8               NaN                               NaN   \n",
       "9               NaN                               NaN   \n",
       "\n",
       "   pft_test_submissions_2023_7 pft_total_minutes_2023_7 n_wifi_days_2023_7  \\\n",
       "0                          NaN                      NaN                NaN   \n",
       "1                          NaN                      NaN                NaN   \n",
       "2                          NaN                      NaN                NaN   \n",
       "3                          NaN                      NaN                NaN   \n",
       "4                          NaN                      NaN                NaN   \n",
       "5                          NaN                      NaN                NaN   \n",
       "6                          NaN                      NaN                NaN   \n",
       "7                          NaN                      NaN                NaN   \n",
       "8                          NaN                      NaN                NaN   \n",
       "9                          NaN                      NaN                NaN   \n",
       "\n",
       "  resource_events_2023_7 n_resource_days_2023_7  \n",
       "0                    NaN                    NaN  \n",
       "1                    NaN                    NaN  \n",
       "2                    NaN                    NaN  \n",
       "3                    NaN                    NaN  \n",
       "4                    NaN                    NaN  \n",
       "5                    NaN                    NaN  \n",
       "6                    NaN                    NaN  \n",
       "7                    NaN                    NaN  \n",
       "8                    NaN                    NaN  \n",
       "9                    NaN                    NaN  \n",
       "\n",
       "[10 rows x 169 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B - Tidiness\n",
    "\n",
    "# Show that each row is a single observation by cross checking duplicates against the identifiers for student, course, and degree hashes\n",
    "duplicates = data2.duplicated(subset=['dni_hash', 'asi_hash', 'anyo_ingreso'])\n",
    "print(\"Number of duplicate rows:\", duplicates.sum())\n",
    "\n",
    "\n",
    "# Show that columns are aptly named\n",
    "print('='*50)\n",
    "print(data2.columns)\n",
    "print('='*50)\n",
    "print(data2.dtypes) # note that for now, dtypes are often objects because pandas interprets the comma usage in certain numbers as a string most likely\n",
    "\n",
    "# Show a preview of what the data looks like, demonstrating that columns are properly named, there are no overlapping values, and columns are generally meaningful\n",
    "print('='*50)\n",
    "data2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape (rows, columns): (159173, 169)\n",
      "Number of observations of student-course-year (rows): 159173\n",
      "Number of variables (columns): 169\n"
     ]
    }
   ],
   "source": [
    "# C - Size of Dataset\n",
    "print(\"Dataset shape (rows, columns):\", data2.shape)\n",
    "print(\"Number of observations of student-course-year (rows):\", data2.shape[0])\n",
    "print(\"Number of variables (columns):\", data2.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As also mentioned in the paper connected to this dataset, there is a high systematic relationship in the missingness of much of the data, as well as a large portion of columns that have a lot of missing data. This is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pft_test_submissions_2023_7</th>\n",
       "      <td>159148</td>\n",
       "      <td>99.984294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pft_assignment_submissions_2023_7</th>\n",
       "      <td>158800</td>\n",
       "      <td>99.765664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es_retitulado</th>\n",
       "      <td>158630</td>\n",
       "      <td>99.658862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total1</th>\n",
       "      <td>157656</td>\n",
       "      <td>99.046949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es_adaptado</th>\n",
       "      <td>156916</td>\n",
       "      <td>98.582046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rendimiento_cuat_a</th>\n",
       "      <td>12207</td>\n",
       "      <td>7.669014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rendimiento_cuat_b</th>\n",
       "      <td>9051</td>\n",
       "      <td>5.686266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rendimiento_total</th>\n",
       "      <td>8819</td>\n",
       "      <td>5.540513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estudios_m_hash</th>\n",
       "      <td>918</td>\n",
       "      <td>0.576731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estudios_p_hash</th>\n",
       "      <td>918</td>\n",
       "      <td>0.576731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   missing_count  missing_pct\n",
       "pft_test_submissions_2023_7               159148    99.984294\n",
       "pft_assignment_submissions_2023_7         158800    99.765664\n",
       "es_retitulado                             158630    99.658862\n",
       "total1                                    157656    99.046949\n",
       "es_adaptado                               156916    98.582046\n",
       "...                                          ...          ...\n",
       "rendimiento_cuat_a                         12207     7.669014\n",
       "rendimiento_cuat_b                          9051     5.686266\n",
       "rendimiento_total                           8819     5.540513\n",
       "estudios_m_hash                              918     0.576731\n",
       "estudios_p_hash                              918     0.576731\n",
       "\n",
       "[124 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D - Missing Data Exploration\n",
    "# Basic exploratory analysis on the missing data as porportions and counts\n",
    "missing_counts = data2.isnull().sum()\n",
    "missing_percent = (missing_counts / len(data2)) * 100\n",
    "\n",
    "missing_df = pd.concat([missing_counts, missing_percent], axis=1)\n",
    "missing_df.columns = ['missing_count', 'missing_pct']\n",
    "\n",
    "missing_df = missing_df[missing_df['missing_count'] > 0].sort_values(by='missing_pct', ascending=False)\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "campus_hash\n",
       "1398b376fdcce25c    88.458559\n",
       "297c138806bdb5dd    87.812500\n",
       "9103a6c82e355433    85.704161\n",
       "3ca0e4af1c44f084    84.429455\n",
       "7b778e4c1d1f33c9    83.958427\n",
       "0f01a84bff1b2bf4    82.044018\n",
       "60f19cd67252161d    79.815005\n",
       "85ff657216cc9b54    79.775281\n",
       "1a9d786be0ff0bfe    79.341426\n",
       "6781b441c78d2643    78.329399\n",
       "48c6e3d042649ef6    77.824773\n",
       "234001f5d5f1eca4    77.424844\n",
       "f9418773503e50b6    77.080491\n",
       "47cfe5eb8ada0e74    76.700434\n",
       "79df3742da86cfd4    76.659119\n",
       "40f5b57b09f073ed    76.653696\n",
       "86348ea0bf50ebf0    75.927487\n",
       "4e808094851fc2ea    75.469381\n",
       "16a36e86f6fed5d4    75.291622\n",
       "e984139bcc2c5043    75.257732\n",
       "f32b702fba23083f    74.931880\n",
       "5d9d4510699dac58    74.718222\n",
       "911ac1b13dac6fe9    73.259053\n",
       "ddf9288fd8062579    72.413793\n",
       "0672d49fe5a7035e    72.110665\n",
       "eb074cd8374ba297    71.810089\n",
       "f2a369a3b17169d7    71.753555\n",
       "52025890fa603dbc    70.521364\n",
       "2f4c06aba0f9a393    70.130678\n",
       "0448d563bf72277a    70.024096\n",
       "8138689887e6817e    68.799798\n",
       "c8361f9b468e68c8    65.359477\n",
       "e4f95d56d90df35e    64.516339\n",
       "f01a95a004153cb8    63.482414\n",
       "474ffa8c8cb7e88e    63.478261\n",
       "e176f557f5261788    62.251149\n",
       "5abe6ed555720a3e    61.641221\n",
       "8b336cdf52ac9b75    59.274194\n",
       "dae96fc0046a5ae1    52.682927\n",
       "bc5d84bed7dee3e1    38.461538\n",
       "Name: n_wifi_days_2023_7, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D - Missing Data Exploration\n",
    "# A deeper dive into why some data is missing in the way it is\n",
    "# Let us take a look at the missing wifi monitoring usage by campus\n",
    "data2.groupby('campus_hash')['n_wifi_days_2023_7'] \\\n",
    "     .apply(lambda x: x.isnull().mean()*100) \\\n",
    "     .sort_values(ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly demonstrates that some campuses such as `bc5d84bed7dee3e1` have a very comparitively low missing percentage (38%) of their students' wifi utilization, while others, like `1398b376fdcce25c` have a very high missing percentage, around 88%. This clearly shows a systematic disparity in certain campus's ability to report such data. While every inconsistency cannot be described due to the sheer size of this dataset, this small subsample shows how there is a large systematic reason for certain data being missing. This is explored in a little more depth in the accompanying paper, but on a theoretical basis, because this dataset was compiled from various databases and resources and then homogenized, inconsistencies are bound to show. Furthermore, certain courses may be more open to utilizing LMS tools or adopting digital platforms for their education, resulting in systematic missingness in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Discovery\n",
    "\n",
    "The accompanying paper describes that during the data anonymization of students, suspicious variables were dealt with to protect anonymity. For example, if a certain aggregation of variables could identify a student, this was deleted. Furthermore duplicate entries were deleted. Furthermore, a general check to ensure data types remained consistent and that value ranges for data was well within the expected distribution accross datasets was conducted. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "\n",
    "This dataset features a high rate of missingness. As such, the general rule for now that we chose to go with was to delete any columns with a high threshold of missingness. In this case, we chose to drop the columns with more than 90% overall missingness (this means dropping around 30 columns), as even if this data may be useful, the sheer proportion of missing data would make it less impactful. While agressive, this will help us narrow down our scope for our final project. A test also revealed that attempting a super agressive drop of all rows with any sort of missing data would cut the dataset to only 162 entries, so this is also not used. However, entries with all NA entries were deleted. Another notable feature is that the csv was ';' deliminated and utilized commas as decimals, which is quite typical of much of Europe. As such, numbers are cleaned into decimal format and converted to float/int. \n",
    "\n",
    "For specific column-based adjustments, a few rules were established to deal with missingness:\n",
    "\n",
    "1) Leave identifying hashes alone\n",
    "2) Leave demographic/enrollment data alone\n",
    "3) Fill credit/coursework work columns as 0 for NA entries\n",
    "4) Fill activity/practical work columns as 0 for NA entries\n",
    "5) Fill LMS/Wifi/Digital Engagement columns as 0 for NA entries\n",
    "\n",
    "This was done because demographics, enrollement data, and identifying hashes being empty are likely a result of truely missing data. However, the other categories can be attributed to simply the absence of the student doing said column. For example, no entry for credits enrolled for a specific semester and for a specific courses may just mean that the student didn't take that course. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dni_hash</th>\n",
       "      <th>tit_hash</th>\n",
       "      <th>asi_hash</th>\n",
       "      <th>anyo_ingreso</th>\n",
       "      <th>tipo_ingreso</th>\n",
       "      <th>nota10_hash</th>\n",
       "      <th>nota14_hash</th>\n",
       "      <th>campus_hash</th>\n",
       "      <th>estudios_p_hash</th>\n",
       "      <th>estudios_m_hash</th>\n",
       "      <th>...</th>\n",
       "      <th>resource_events_2023_5</th>\n",
       "      <th>n_resource_days_2023_5</th>\n",
       "      <th>pft_events_2023_6</th>\n",
       "      <th>pft_days_logged_2023_6</th>\n",
       "      <th>pft_visits_2023_6</th>\n",
       "      <th>pft_total_minutes_2023_6</th>\n",
       "      <th>n_wifi_days_2023_6</th>\n",
       "      <th>resource_events_2023_6</th>\n",
       "      <th>n_resource_days_2023_6</th>\n",
       "      <th>n_wifi_days_2023_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>4596fcf257c4</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>81f4b5a1d0a8</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>442fcac005ed</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>3dc87ab71825</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>319636fc9270</td>\n",
       "      <td>620c9c332101</td>\n",
       "      <td>677c622c0bfb</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.456</td>\n",
       "      <td>e4f95d56d90df35e</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dni_hash      tit_hash      asi_hash  anyo_ingreso tipo_ingreso  \\\n",
       "0  319636fc9270  620c9c332101  4596fcf257c4        2012.0          NAP   \n",
       "1  319636fc9270  620c9c332101  81f4b5a1d0a8        2012.0          NAP   \n",
       "2  319636fc9270  620c9c332101  442fcac005ed        2012.0          NAP   \n",
       "3  319636fc9270  620c9c332101  3dc87ab71825        2012.0          NAP   \n",
       "4  319636fc9270  620c9c332101  677c622c0bfb        2012.0          NAP   \n",
       "\n",
       "   nota10_hash  nota14_hash       campus_hash estudios_p_hash estudios_m_hash  \\\n",
       "0          NaN        9.456  e4f95d56d90df35e               F               L   \n",
       "1          NaN        9.456  e4f95d56d90df35e               F               L   \n",
       "2          NaN        9.456  e4f95d56d90df35e               F               L   \n",
       "3          NaN        9.456  e4f95d56d90df35e               F               L   \n",
       "4          NaN        9.456  e4f95d56d90df35e               F               L   \n",
       "\n",
       "   ... resource_events_2023_5 n_resource_days_2023_5 pft_events_2023_6  \\\n",
       "0  ...                    0.0                    0.0               0.0   \n",
       "1  ...                    0.0                    0.0               0.0   \n",
       "2  ...                    0.0                    0.0               0.0   \n",
       "3  ...                    0.0                    0.0               0.0   \n",
       "4  ...                    0.0                    0.0               0.0   \n",
       "\n",
       "   pft_days_logged_2023_6 pft_visits_2023_6  pft_total_minutes_2023_6  \\\n",
       "0                     0.0               0.0                       0.0   \n",
       "1                     0.0               0.0                       0.0   \n",
       "2                     0.0               0.0                       0.0   \n",
       "3                     0.0               0.0                       0.0   \n",
       "4                     0.0               0.0                       0.0   \n",
       "\n",
       "  n_wifi_days_2023_6  resource_events_2023_6  n_resource_days_2023_6  \\\n",
       "0                0.0                     0.0                     0.0   \n",
       "1                0.0                     0.0                     0.0   \n",
       "2                0.0                     0.0                     0.0   \n",
       "3                0.0                     0.0                     0.0   \n",
       "4                0.0                     0.0                     0.0   \n",
       "\n",
       "  n_wifi_days_2023_7  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F - Data Cleaning\n",
    "\n",
    "data_cleaned_2 = data2.copy()\n",
    "\n",
    "# Step 1: Drop columns with >90% missingness\n",
    "threshold = 0.90\n",
    "data_cleaned_2 = data_cleaned_2.loc[:, data_cleaned_2.isna().mean() < threshold].copy()\n",
    "\n",
    "# Step 2: Drop rows that are all NA\n",
    "data_cleaned_2 = data_cleaned_2.dropna(axis=0, how='all')\n",
    "\n",
    "# Step 3: Convert comma-based numbers to floats\n",
    "for col in data_cleaned_2.select_dtypes(include='object').columns:\n",
    "    try:\n",
    "        data_cleaned_2[col] = data_cleaned_2[col].str.replace(',', '.').astype(float)\n",
    "    except:\n",
    "        pass  # leave non-numeric columns as object\n",
    "\n",
    "# Step 4: Fill NA with 0 for predetermined count/activity columns\n",
    "fillna_cols = [col for col in data_cleaned_2.columns \n",
    "               if any(x in col for x in ['n_wifi_days', 'resource_events', 'n_resource_days', \n",
    "                                         'pft_', 'actividades', 'total1', 'cred_mat', 'cred_sup'])]\n",
    "\n",
    "for col in fillna_cols:\n",
    "    if pd.api.types.is_numeric_dtype(data_cleaned_2[col]):\n",
    "        data_cleaned_2[col] = data_cleaned_2[col].fillna(0)\n",
    "\n",
    "data_cleaned_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cleaned dataframe: (159173, 135)\n",
      "\n",
      "Data types:\n",
      "float64    115\n",
      "object      13\n",
      "int64        7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cleaned dataset saved to: data/02-processed/university_dropout_cleaned_2022.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of cleaned dataframe:\", data_cleaned_2.shape)\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(data_cleaned_2.dtypes.value_counts())\n",
    "\n",
    "processed_file_path = os.path.join(PROCESSED_DATA_DIR, 'university_dropout_cleaned_2022.csv')\n",
    "data_cleaned_2.to_csv(processed_file_path, index=False, sep=';')\n",
    "\n",
    "print(f\"\\nCleaned dataset saved to: {processed_file_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Collection\n",
    " - [ ] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    "> We will have to read through the paper that originally collected this data and determine whether the data was appropriately collected and whether or not students were given adequate warning as to what was being collected.\n",
    "\n",
    " - [ ] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "\n",
    "> The data was collected by serveral researchers in Portugal for the purposes of their research as such we will have to consider the bias that the researchers possibly could have introduced through the data collection and the biases that may originate from the data being collected in portugal.\n",
    "\n",
    " - [ ] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "\n",
    "> Although we currently believe the data we have is anonymous we will need to re-examine the source and determine if there is any additional information that needs to be censored.\n",
    "\n",
    " - [ ] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "> We will have to perform analysis where we include and exclude gender, our protected groups, to see if doing so results some form of algorithmic bias so that we can hopefully correct for it and provide an unbiased collection of data.\n",
    "\n",
    "\n",
    "### B. Data Storage\n",
    " - [ ] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    "\n",
    "> Since we are not generating any new data and instead using an already created data set there will not be anything for us to hide, although we could hide the conclusions of our data at the end of the analysis.\n",
    "\n",
    " - [ ] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    "\n",
    "> The data was already collected by others so we cannot support this for the original participants.\n",
    "\n",
    " - [ ] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "> The data was already collected by others so we cannot support this for the original participants.\n",
    "\n",
    "### C. Analysis\n",
    " - [ ] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "\n",
    "> While we intend for our analysis to be qualitative, we could reach out to experts in education inequity to get a better handle of how accurate our analysis is from their persepctive.\n",
    "\n",
    " - [ ] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    "\n",
    "> Since the data has many data points that are discrete, many that maybe would have been better served being continuous, we will take specific caution towards determining which of these can be used and determining how best to interpret and use them.\n",
    "\n",
    " - [ ] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    "\n",
    "> We will have to ensure that different data points are correctly weighted when determining such that we are not forcing our analysis towards a particulur conclusion.\n",
    "\n",
    " - [ ] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    "\n",
    "> Since the data does not have any specific PII, we do not anticipate having to do anything specific for our analysis.\n",
    "\n",
    " - [ ] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    "> We intend to have a very well formatted and easily readable jupyter notebook to ensure that it is easy to see what we did thus making it easily auditable.\n",
    "\n",
    "### D. Modeling\n",
    " - [ ] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    "\n",
    "> We will have to be careful with the careers or the parents as these may be proxies that result in us finding that socioeconomic status is the sole determinant of success.\n",
    "\n",
    " - [ ] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    "\n",
    "> We will have to test to see if the model is appropriatley fair accross the binary grouping that they have in the data like rural vs urban addresses which do not actually make up an address and rather just have a one value for rural and another for urban.\n",
    "\n",
    " - [ ] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    "\n",
    "> Since we are intending to optimize to find out what factors can be used as predictors to determine if a students chance of dropping out we need to be cautious that this can result in false positives and negatives, if our model places certain students in the dropout or graduate pile this could change how money is spent in a school which we would like to avoid.\n",
    "\n",
    " - [ ] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    "\n",
    "> We hope to make the model very clear and to explain all of the analysis that we do in the jupyter notebook so that we can go back and understand and justify the decision that the model made.\n",
    "\n",
    " - [ ] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "> We will have to ensure that we communicate the various limitations that are present with our data and analysis so that who ever desires to look at our model understands the potential flaws in our analysis and modeling.\n",
    "\n",
    "### E. Deployment\n",
    " - [ ] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    "\n",
    "> We do not yet have a clear plan for how to monitor the model after it is deployed but, if it were to be used, the users would likely have to be careful with how they use it when modifying school programs and spending.\n",
    "\n",
    " - [ ] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    "\n",
    "> No we have not discussed this, but it would be difficult to address if it were to happen since we do not know the respondants and are geographically very far away from them.\n",
    "\n",
    " - [ ] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    "\n",
    "> We think that we would be able to just turn off the script. In the end our analysis will not continue to aggregate more data so we do not anticipate any issues that require deleting the model.\n",
    "\n",
    " - [ ] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n",
    "\n",
    "> We have yet to take steps towards this, but we need to ensure that the models results cannot be misinterpreted and abused for someone elses purpose.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our primary form of communication is Discord. We expect all members to respond to and/or acknowledge all members' messages within one day. We plan to meet once a week, either in person or virtually, on Monday afternoons.\n",
    "We expect all members to maintain a respectful and polite tone when communicating with others. Don't be mean, even if there are disagreements. We want to keep an open mind, value everyone's opinions equally, and be proactive in brainstorming solutions for the good of the team. For example, if there are conflicting perspectives, we can communicate our opinions by saying \"I don't think moving forward with X is within our group's best interest because of Y. Instead, we should explore Z.\"\n",
    "\n",
    "* Ideally, we want to make unanimous decisions. However, this is not always possible, so we will default to majority vote rules. If a member does not reply or acknowledge a proposal/message within a day, we can move forward with their input. Team members can react to Discord messages as a form of acknowledgement, especially if they're unable to respond immediately.\n",
    "\n",
    "* Every member will get first-hand experience pertaining to all aspects of our project. Having members do a little bit of everything will ensure that we are all able to develop our skills individually. We will delegate tasks during our weekly meetings and send a message in our \"to-do\" channel on Discord.\n",
    "\n",
    "* If there any issues, we expect each other to speak up EARLY before the deadline. As a general rule of thumb, we expect members to reach out at least a day or two PLUS the expected time it takes to complete the specific task if there are any issues or concerns.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tentative timeline that is subject to change throughout the rest of the quarter\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/26  |  3 PM | Reviewed Lecture slides and any information related to our project  | Determine best form of communication; Introduce Ourselves; Review previous projects; Begin brainstorming possible project ideas  | \n",
    "| 2/2  |  2 PM |  Brainstorm Ideas For Final Project | Discuss ideal dataset(s) and project ideas; Draft project proposal/Assign Individual parts;  | \n",
    "| 2/9  | 2 PM| Finish and finalize project proposal  | Discuss Wrangling and possible analytical approaches; Discuss overall organiation of project and procedures; Work on data  |\n",
    "| 2/16 or 2/18  | 2 PM  | Review dataset and have it prepared for analysis | Work on Data Checkpoint; Discuss Analysis Plan   |\n",
    "| 2/23  | 2 PM  | Finish data checkpoint and all things related to data | Work on analysis of our data  |\n",
    "| 3/2  | 2 PM  | Complete analysis; Draft results/conclusion/discussion | Discuss/edit full project |\n",
    "| 3/9  | 2PM  | Work on Final Project | Finishing touches; Turn in Final Project; Group Project Surveys |\n",
    "| 3/16(?)  | 2PM  | Work on Final Project | Buffer Day if necessary |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
